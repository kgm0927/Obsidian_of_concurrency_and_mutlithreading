
이제 Go에서 동시성을 활용하는 일반적인 패턴을 배웠으므로, 이러한 패턴들을 실제로 구성해 크고 구성 가능하면서도 확장할 수 있는 시스템을 작성해보자.

5장에서는 단일 프로세스 내에서 동시 작업의 규모를 조정하는 방법을 논의하고, 둘 이상의 프로세스를 처리할 때 동시성이 어떻게 작용하는지 살펴본다.

---

# 에러 전파


동시 코드, 특히 분산 시스템을 사용하면 시스템에서 문제가 발생하기 쉽고, 문제의 원인을 쉽게 파악할 수 없다. 시스템 전반에 걸쳐 문제가 전파되는 방식과 최종 사용자가 어떻게 대처해야 하는지를 신중하게 고려해야 자신이나 팀, 또는 사용자의 고통을 덜어줄 수 있다.

143 페이지의 "[[Go의 동시성 패턴#에러처리|에러 처리]]"에서 고루틴에서 에러를 전파하는 방법은 논의했지만, 그러한 에러가 어떤 모습이어야 하는지나, 크고 복잡한 시스템에서 에러가 어떻게 전달돼야 하는지에 대해서는 이야기하지 않았다. 이번에는 에러 전파의 철학에 대해 논의해보자. 다음은 동시성 시스템에서 에러를 처리하기 위한 독창적인 프레임워크다.

많은 개발자가 에러 전파가 시스템의 흐름에서 부수적이거나 "별개"의 요소라고 생각하는 실수를 범한다. 데이터가 시스템을 통해 흘러가는 방법에 대해서는 신중하게 고려하지만, 에러는 별다른 생각없이 용인되고 불시에 스택을 빠져나가 결국은 사용자 앞에 버려지는 것이다.

Go는 사용자가 호출 스택의 모든 프레임에서 에러를 처리하도록 강요함으로써 이 나쁜 습관을 고치려고 시도했지만, 시스템의 제어 흐름이 에러를 2등 시민으로 간주하는 것은 흔한 일이다. 조금만 미리 고민하고 최소한의 부담만 감수하면 에러 처리를 시스템의 자산으로 만들고 사용자에게 즐거움을 줄 수 있다.

먼저 에러가 무엇인지 살펴보자. 언제 에러가 발생하며, 에러를 통해 얻는 이점은 무엇일까?

==**에러**는 시스템이 사용자가 명시적으로 또는 암시적으로 요청한 작업을 수행할 수 없는 상태에 들어갔음을 나타낸다.== 이 때문에 에러는 몇 가지 중요한 정보를 전달해야 한다.


- 발생한 사건

이것은 **"디스크가 가득 찼다"**,**"소켓이 닫혔다"** 또는 **"자격이 만료되었다"** 와 같은 정보를 포함한 에러의 유형이다. 이 정보는 에러를 발생시킨 것이 무엇이든 간에 암시적으로 생성될 수 있으며, 사용자에게 도움이 될 만한 컨텍스트를 추가시킬 수 있다.


- 발생한 장소 및 시점

==에러에는 항상 호출이 시작된 메서드부터 시작해 에러가 인스턴스화된 위치로 끝나는 전체 스택 트레이스가 포함돼야 한다.== 스택 트레이스가 에러 메시지에 포함돼서는 안 되지만(이에 대해서는 잠시 후에 다룬다), ==스택 위쪽에 에러를 처리할 때는 쉽게 접근할 수 있어야 한다.==

또한 에러에는 실행 중인 컨텍스트와 관련된 정보가 포함돼야 한다. 예를 들어, ==분산 시스템에서는 에러가 발생한 시스템을 식별할 수 있는 방법이 있어야 한다.== 나중에 시스템에서 발생한 일을 이해하려고 할 때 이 정보는 매우 중요하다.


- 사용자 친화적인 메시지

==사용자에게 표시되는 메시지는 시스템 및 시스템의 사용자에 맞춰 조정해야 한다.== 이때 앞의 두 가지에 대한 간략하면서도 유의미한 정보만을 포함해야 한다. 친화적인 메시지는 인간 중심적이면서 문제가 일시적인지 여부를 알려주는 한 줄 정도의 문구여야 한다.

- 사용자가 추가적인 정보를 얻을 수 있는 방법

어떤 시점에서 누군가는 에러가 발생했을 때 일어난 일을 자세히 알고 싶어 할 것이다==. 사용자에게 표시되는 에러는 에러가 기록된 시간이 아닌 에러의 발생시간, 에러가 생성됐을 때의 전체 스택 트레이스 등 에러의 전체 정보를 표시하는 로그에 상호 참조될 수 있는 ID를 제공해야 한다.== 또한 버그 추적 시스템에서 비슷한 문제를 집계하는 데 도움이 되는 스택 트레이스의 해시 값을 포함하는 방식도 유용할 수 있다.


기본적으로 아무런 개입 없이 이러한 정보가 모두 에러에 포함되지는 않는다.

따라서 이런 정보 없이 사용자에게 전파되는 에러는 실수이며 버그라 할 수 있다. 이 문제를 해결하기 위해 에러에 대해 생각할 수 있는 범용적인 프레임워크에 대해 고민할 필요가 있다. 모든 에러는 다음 두 가지 범주 중에 하나로 분류할 수 있다.

- 버그
- 알려진 예외적인 경우(예: 네트워크 연결 단절, 디스크 쓰기 실패 등)

==**버그**는 사용자가 **시스템에 맞춰 정의하지 않는 에러** 또는 "**처리되지 않은(raw)**"에러다.== 때로는 의도적인 경우도 있다. 시스템 출시 후 처음 몇 번의 반복 동안은 이미 알려진 예외에 해당하는 에러를 사용자에게 알릴 수도 있었다. **때로는 버그가 우연히 발생하기도 한다.** 그러나 여기서 제시한 접근법에 동의한다면, 처리되지 않은 에러는 항상 버그다. 이렇게 구분하면 에러를 전파하는 방법, 시간이 지남에 따라 시스템을 확대하는 방법 및 최종적으로 사용자에게 표시할 내용을 결정할 때 유용하다.

여러 개의 모듈을 가진 대형 시스템을 상상해보자.

``` mermaid
flowchart LR

node1(CLI 구성요소)-->node2(중개자 구성 요소)-->node3(하위 구성 요소)
```

그리고 "하위 구성 요소"(Low Level Component)에서 에러가 발생했고 올바른 형식(well-formed)의 에러를 만들어 스택의 위쪽에 전달했다고 가정해보자. ==**"하위 구성 요소"** 컨텍스트에서 이 에러는 올바른 형식으로 간주될 수 있지만, 시스템의 컨텍스트 내에서는 그렇지 않을 수도 있다.== 각 구성 요소의 경계에서, 들어오는 모든 에러는 코드가 포함된 구성 요소에 올바른 형식의 에러로 포장돼야 한다는 입장을 취해보자. 예를 들어 **"중개자 구성 요소"**(Intermediary Compnent)에서 **"하위 구성 요소"** 의 코드를 호출했을 때 에러가 발생했을 수 있다.

``` go
func PostReport(id string) error{
result,err:=lowlevel.DoWork()

if err !=nil{
  if _,ok:=err.(lowlevel.Error);err{ // #1
  err=WrapErr(err,"cannot post report with id %q",id) // #2
  }
return err

}
// ..
}
```

1. 여기서는 올바른 형식의 에러를 받았는지 확인한다. 그렇지 않은 경우 버그를 나타내기 위해 잘못된 형식의 에러를 스택의 위쪽으로 전달한다.
2. 여기서는 가상 함수를 호출해 전달받은 에러를 모듈에 대한 정보와 함께 포장하고 새로운 타입을 부여한다. 에러를 감싸는 것은, 이 컨텍스트 내에서 사용자에게 중요하지 않을 수 있는 저수준의 세부 정보를 숨기는 것이 포함될 수 있다.

==에러가 발생한 근본적인 원인이 어디인지에 대한 세부 정보(예: 고루틴, 기기, 스택 트레이스 등)는 에러가 처음 인스턴스화될 때 채워지지만==, ==아키텍처는 모듈의 경계에서 이 에러를 적절한 정보로 채워진 모듈의 에러 타입으로 변환할 것을 지시한다.== **이제 모듈의 에러 타입이 아닌 상태로 모듈을 벗어나는 모든 에러는 잘못된 형식으로 간주될 수 있으며, 버그가 될 수 있다.** 공개(public) 함수나 메서드와 같은 자기 모듈의 경계에서, 혹은 코드에 유용한 컨텍스트를 추가할 수 있는 경우에만 에러를 이 방식으로 감싸면 된다는 점에 대해서 유의하자. 이로 인해 보통은 코드에서 에러를 감쌀 필요가 없게 된다.

이 입장을 취하면 시스템은 매우 유기적으로 성장할 수 있다. ==전달받은 에러가 올바른 형태라고 확신할 수 있으며, 최종적으로 에러가 모듈 밖으로 나가는 방식을 확정할 수 있다.== ==에러 정확성이 곧 시스템의 새로운 속성이 된다.== 우리는 잘못된 형식의 에러를 명시적으로 처리함으로써 시스템이 최초에 완벽한 상태임이 받아들이고, 이를 기반으로 시간이 지나면서 발생할 수 있는 실수들을 포착해 수정할 수 있는 프레임워크를 보유하게 됐다. 타입이나 사용자에서 보여주는 내용을 바탕으로 잘못된 형식의 에러를 분명하게 기술할 수 있다.


==앞서 확립한 프레임워크에 따르며, 모든 에러는 가능한 한 많은 정보와 함께 기록돼야 한다.== 그러나 사용자에게 에러를 표시하는 시점에서 버그와 알려진 경계 케이스의 차이가 발생한다.

사용자가 대하는 코드가 올바른 형식의 에러를 받으면, 코드의 모든 단계에서 에러 메시지를 작성하는 데 주의를 기울였다는 사실을 확인할 수 있으며, 사용자가 볼 수도 있도록 간단히 기록하고 출력할 수 있다. 올바른 타입의 에러를 보면서 얻을 수 있는 자신감은 간과할 수 없다.

잘못된 형식의 에러, 즉 버그가 사용자에게 전파될 때도 이 에러를 기록해야 하지만, 사용자에게는 예상치 못한 일이 발생했다는 친숙한 메시지가 표시된다. ==시스템에서 자동 에러 보고를 지원하는 경우, 이 에러를 버그로 보고해야 한다.== 자동 에러 보고를 지원하지 않는 경우라면 사용자에게 버그 보고서를 제출해달라고 제안할 수 있다. 잘못된 형식의 에러에 실제로 유용한 정보가 포함돼 있을 수 있지만 이는 확신할 수 없으며, 확실한 것은 에러가 제대로 가공되지 않았다는 것 뿐이다. 그렇기 때문에 무슨 일이 일어났는지에 대해서 설명하는, 인간을 배려하지 않는 메시지를 퉁명스럽게 표시할 수 밖에 없다.

==올바른 형식의 에러나 잘못된 형식의 에러 모두 메시지에 로그 ID를 포함시켜, 사용자가 더 많은 정보를 원할 경우 다시 참조할 수 있는 항목을 제공해야 한다는 점을 기억하라.== 따라서 버그에 유용한 정보가 포함되어 있더라도 호기심 많은 사용자는 여전히 조사할 방법이 있다.

완전한 예제를 살펴보자. 이 예제가 극단적으로 안정적인 것은 아니다. 예를 들어 에러 타입이 지나치게 단순할 수도 있다. 호출 스택은 선형적이기 때문에 모듈 경계에만 에러를 감싸면 된다는 사실이 애매해졌다. 또한 책에서 다른 패키지의 함수라는 것을 표현하기가 어렵기 때문에 여기서는 가상 함수를 사용할 것이다.

먼저, 앞서 논의한 올바른 형식의 에러의 모든 측면을 포함할 수 있는 에러 타입을 작성해보겠다.


``` go

package main

  

import (
    "fmt"
    "runtime/debug"
)

  

type MyError struct {
    Inner      error
    Message    string
    StackTrace string
    Misc       map[string]interface{}
}

  

func wrapError(err error, messagef string, msgArgs ...interface{}) MyError {

    return MyError{
        Inner:   err, // #1
        Message: fmt.Sprintf(messagef,msgArgs...),
        StackTrace: string(debug.Stack()), // #2
        Misc: make(map[string]interface{}), // #3
    }

}

func (err MyError) Error() string {
    return err.Message
}
```

1. ==여기서는 감싸고 있는 에러를 저장한다.== 우리는 무슨 일이 발생했는지 조사하고자 하는 경우에 언제든 가장 낮은 수준의 에러로 되돌아갈 수 있기를 원한다.
2. ==이 행은 에러가 생성될 때의 스택 트레이스를 기록한다.== 보다 정교한 에러 타입에서는 wrapError에서 스택 프레임을 생략할 수도 있다.
3. ==여기서는 잡다한 정보를 저장하기 위한 잡동사니 보관함을 만든다.== 여기에서 동시성 ID, 스택 트레이스의 해시 또는 에러 진단에 도움이 되는 기타 상황별 정보를 저장할 수 있다.


다음으로 lowlevel 모듈을 만든다.

``` go
// "lowlevel" 모듈

type LowLevelErr struct{
    error
}

  

func isGloballyExec(path string) (bool,error) {

    info,err:=os.Stat(path)
    if err != nil {
        return false,LowLevelErr{(wrapError(err,err.Error()))} // #1
    }
    return info.Mode().Perm()&0100==0100,nil
}
```

1. 여기서는 os.Stat 호출로 인해 발생한 원시 에러를 맞춤형 에러로 감싼다. 이 경우, 이 에러에서 나오는 메시지도 괜찮기 때문에 굳이 가리지 않는다.

그러면 lowlevel 패키지에서 함수들을 호출하는 intermediate라는 또 다른 모듈을 생성해보자.


``` go

// "intermediate" 모듈

type intermediateErr struct {
    error
}

  

func runJob(id string) error {

    const joBinPath = "/bad/job/binary"
    isExecutable, err := isGloballyExec(joBinPath)

    if err != nil {
        return err // #1
    } else if isExecutable == false {
        return wrapError(nil, "job binary is not executable")
    }
    return exec.Command(joBinPath, "--id="+id).Run() // #1

}
```

1. 여기서는 lowlevel 모듈에서 발생한 에러들을 전달한다. 자체 버그 타입으로 감싸지지 않은, 다른 모듈에서 전달된 에러는 버그로 간주하겠다는 아키텍처적인 결정으로 인해 이것은 나중에 문제가 될 수 있다.

마지막으로 intermediate패키지의 함수를 호출하는 취상위의 main 함수를 만든다. 프로그램 중에서 사용자에게 보여지는 부분이 바로 이곳이다.

``` c++
func handleError(key int, err error, message string) {
    log.SetPrefix(fmt.Sprintf("[logID:%v]:", key))
    log.Printf("%#v", err) // #3
    fmt.Printf("[%v] %v", key, message)
}

  

func main() {

    log.SetOutput(os.Stdout)
    log.SetFlags(log.Ltime | log.LUTC)
    err := runJob("1")
    if err != nil {

        msg := "There was an unexpected issue; please report this as a bug."

        if _, ok := err.(intermediateErr); ok { // #1
            msg = err.Error()
        }

        handleError(1, err, msg) // #2

    }

}
```

1. 여기서는 에러가 예상하고 있던 타입인지 확인한다. 예상하던 타입이라면 올바른 형식의 에러라는 것을 알기 때문에 사용자에게 메시지를 전달할 수 있다.
2. 이 행에서는 로그 및 에러 메시지에 ID 1을 붙인다. 간단히 ID를 일정하게 증가시키거나 혹은 고유한 ID를 만들기 위해 GUID를 사용할 수 있다.
3. 여기서는 누군가가 무슨 일이 일어났는지 파헤쳐야 할 경우에 대비해 전체 에러를 기록한다.


이를 실행하면 다음과 같은 내용을 포함한 로그 메시지를 얻게 될 것이다.


``` shell
PS C:\Users\kgm09\goproject\src> go run main.go
[logID:1]:13:59:05 main.LowLevelErr{error:main.MyError{Inner:(*fs.PathError)(0xc000022180), Message:"CreateFile /bad/job/binary: The system cannot find the path specified.", StackTrace:"goroutine 1 [running]:\nruntime/debug.Stack()\n\tC:/Users/kgm09/go/src/runtime/debug/stack.go:24 +0x5e\nmain.wrapError({0xca66b8, 0xc000022180}, {0xc000010230?, 0xc00007bda8?}, {0x0?, 0x10?, 0xc610a0?})\n\tC:/Users/kgm09/goproject/src/main.go:22 +0x75\nmain.isGloballyExec({0xc7ec98?, 0x13?})\n\tC:/Users/kgm09/goproject/src/main.go:40 +0x65\nmain.runJob({0xc7cf78, 0x1})\n\tC:/Users/kgm09/goproject/src/main.go:53 +0x3b\nmain.main()\n\tC:/Users/kgm09/goproject/src/main.go:71 +0x4d\n", Misc:map[string]interface {}{}}}

```

그리고 표준 출력에는 다음과 같이 출력될 것이다.

```shell

[1] There was an unexpected issue; please report this as a bug.
```

이 에러의 경로 어딘가에서 제대로 처리하지 않았음을 알 수 있다. 이 에러 메시지는 사람이 처리하기에 적합하다고 확신할 수 없기 때문에 예상치 못한 일이 발생했다는 간단한 에러 메시지를 출력한다(이 방법론에 따르면 예상치 못한 일이 맞다). intermediate 모듈로 돌아가 보면 그 이유를 알 수 있다. lowlevel 모듈에서는 에러를 감싸지 않았다. 이를 수정하고 결과를 보자.


```go
func runJob(id string) error {

    const joBinPath = "/bad/job/binary"

    isExecutable, err := isGloballyExec(joBinPath)

    if err != nil {
        return intermediateErr{wrapError(
           err, "cannot run job %q: requisite binaries not available",
            id,
        )} // #1

    } else if isExecutable == false {

        return wrapError(
            nil,
            "Cannot run job %q: requisite binaries are not executable",
            id,
        )
    }
    return exec.Command(joBinPath, "--id="+id).Run()
}
```

1. 이번에는 손질된 메시지를 사용해 에러를 원하는 대로 변경한다. 이 경우, 작업이 실행되지 않은 이유가 모듈의 소비자에게 중요하지 않은 정보라고 생각했기 때문에 저수준의 구체적인 내용을 애매하게 숨기고자 했다.

``` go
func handleError(key int, err error, message string) {
    log.SetPrefix(fmt.Sprintf("[logID:%v]:", key))
    log.Printf("%#v", err) // #3
    fmt.Printf("[%v] %v", key, message)
}

  

func main() {

    log.SetOutput(os.Stdout)
    log.SetFlags(log.Ltime | log.LUTC)
    err := runJob("1")

    if err != nil {
        msg := "There was an unexpected issue; please report this as a bug."
        if _, ok := err.(intermediateErr); ok { // #1
            msg = err.Error()
        }
        handleError(1, err, msg) // #2
    }

}
```

수정된 코드를 실행하면 유사한 로그 메시지를 얻게 된다.

```shell

[logID:1]:14:14:20 main.intermediateErr{error:main.MyError{Inner:main.LowLevelErr{error:main.MyError{Inner:(*fs.PathError)(0xc000022180), Message:"CreateFile /bad/job/binary: The system cannot find the path specified.", StackTrace:"goroutine 1 [running]:\nruntime/debug.Stack()\n\tC:/Users/kgm09/go/src/runtime/debug/stack.go:24 +0x5e\nmain.wrapError({0x696738, 0xc000022180}, {0xc000010230?, 0x214913dace8?}, {0x0?, 0x214913d0108?, 0xd0?})\n\tC:/Users/kgm09/goproject/src/main.go:22 +0x75\nmain.isGloballyExec({0x66ec98?, 0x63688a?})\n\tC:/Users/kgm09/goproject/src/main.go:40 +0x65\nmain.runJob({0x66cf78, 0x1})\n\tC:/Users/kgm09/goproject/src/main.go:53 +0x3e\nmain.main()\n\tC:/Users/kgm09/goproject/src/main.go:78 +0x4d\n", Misc:map[string]interface {}{}}}, Message:"cannot run job \"1\": requisite binaries not available", StackTrace:"goroutine 1 [running]:\nruntime/debug.Stack()\n\tC:/Users/kgm09/go/src/runtime/debug/stack.go:24 +0x5e\nmain.wrapError({0x6966d8, 0xc000028080}, {0x676b2e?, 0xc00004a030?}, {0xc00007be58?, 0x0?, 0x3?})\n\tC:/Users/kgm09/goproject/src/main.go:22 +0x75\nmain.runJob({0x66cf78, 0x1})\n\tC:/Users/kgm09/goproject/src/main.go:55 +0x20e\nmain.main()\n\tC:/Users/kgm09/goproject/src/main.go:78 +0x4d\n", Misc:map[string]interface {}{}}}
[1] cannot run job "1": requisite binaries not available
```

그러나 에러 메시지는 정확히 사용자에게 보여주고자 했던 내용이다.

``` shell
[1] cannot run job "1": requisite binaries not available
```

이러한 접근법에 호환되는 에러 패키지들이 있지만, 어떤 에러 패키지를 사용하기로 결정하든 간에 그 패키지를 활용해 이 기법을 구현하는 것은 스스로의 몫이다. 좋은 소식은 이 기법이 유기적이라는 것이다. 최고 수준의 에러 처리를 구상하고 버그와 올바른 형식의 에러를 구분한 다음, 점진적으로 생성되는 모든 에러가 제대로 처리된 것으로 간주되고 있는지 확인하라.



### 시간 초과 및 취소

동시성 코드로 작업할 때 시간 초과(timeout)나 취소(cancellation)가 자주 발생한다. 이 절에서 살펴보겠지만, 시간 초과는 동작을 이해할 수 있는 시스템을 만드는 여러 요소 중 가운데 결정적인 요소이다. 취소는 시간 초과에 따르는 자연스러운 반응이다. 또한 동시에 실행되는 프로세스가 취소될 수 있는 다른 이유에 대해서도 알아볼 것이다.

동시 프로세스가 시간 초과를 지원하기를 원하는 이유가 무엇일까? 다음과 같은 몇 가지 이유가 있다.


###### 시스템 포화

178페이지의 "[[Go의 동시성 패턴#대기열 사용|대기열 사용]]"에서 설명했듯이, **시스템이 포화 상태인 경우** (즉, 가용한 요청 처리 능력을 완전히 보여주는 경우), ==시스템의 경계에 있는 요청이 오랜 시간 후에 처리되는 것 보다는 시간 초과되는 것을 원할 수도 있다.== 어떤 경로를 취할지는 문제 공간에 따라 다르지만, 시간 초과를 발생시키는 경우에 대한 몇 가지 일반적인 지침이 있다.

- 시간 초과되었다고 해도 요청이 반복된 가능성이 낮다.
- 메모리 내 요청을 저장하기 위한 메모리, 영속적인 대기열을 저장하기 위한 디스크 공간 등 요청을 저장할 수 있는 리소스가 부족하다.
- 시간이 지날수록 요청 또는 전송 중인 데이터의 필요성이 줄어든다(뒤에서 이 문제에 대해 논의할 예정). ==요청이 반복될 가능성이 높으면 시스템은 요청을 받아들이고 시간 초과를 처리하는 부담이 늘어날 것이다.== ==이 부담이 시스템의 용량보다 더 커지면 이로 인해 죽음의 나선이 생길 수 있다.== 만약 요청을 대기열에 저장하는 데 필요한 시스템 리소스 자체가 부족하다면 이것은 고려할 가치조차 없다. 그리고 앞의 두 가지 지침을 따른다고 하더라도, 처리할 수 있는 시간이 되기 전에 요청을 처리할 필요성이 사라지는 요청을 저장해야 하는 지점이 있다. 이로 인해 시간 초과를 지원해야 하는 또 다른 이유가 생긴다.

###### 오래된 데이터

때로는 데이터에 처리돼어야만 하는 기간이 있을 수 있다. ==어떤 데이터는 더 적절한 데이터를 사용할 수 있기 전, 혹은 데이터가 만료되기 전에 처리돼야 한다.== ==현재 프로세스가 데이터를 처리하는데, 이 기간보다 더 많은 시간이 소요되면 동시 프로세스를 시간 초과해 취소해야 한다.== 예를 들어, 동시 프로세스가 오랫동안 기다린 후에 요청을 대기열에서 꺼낸다면, 요청이나 그 데이터는 기다리는 동안에 쓸모 없어질 수 없다.

이 기간을 사전에 알고 있다면 context.WithDeadline 또는 context.WithTimeout으로 생성된 동시 프로세스를 전달하는 것이 좋다. 기간을 미리 알 수 없다면 요청이 더 이상 필요하지 않은 경우에 동시 프로세스의 상위 프로세스가 동시 프로세스를 취소할 수 있어야 한다. context.WithCancel은 이 목적에 딱 알맞다.


###### 데드락을 막으려는 시도

대규모 시스템, 특히 분산 시스템에서는 데이터가 어떻게 흘러가는지, 어떤 예외적인 경우가 나타날지 이해하기 어려울 수 있다. 시스템이 데드락에 빠지지 않도록 모든 동시 작업에 시간 초과를 설정하는 것은 불합리하고 권장하지도 않는 방법이다. ==시간 초과 기간과 동시 연산의 실제 수행 시간이 비슷할 필요는 없다.== 시간 초과 기간을 설정하는 목적은 데드락을 막기 위한 것일 뿐이므로, 데드락 상태의 시스템이 사용자 케이스에 맞게 적당한 시간 내에 차단 해제될 만큼만 짧으면 된다.

33페이지의 "[[동시성 소개#데드락, 라이브락, 기아 상태|데드락, 라이브락, 기아 상태]]"절에서, 데드락을 피하기 위해 시간 초과를 설정하려는 시도는 잠재적으로 시스템에서 데드락을 유발하는 문제를 라이브락을 유발하는 문제로 바꿀 수 있다고 했던 내용을 떠올려보자. 그러나 ==대형 시스템에서는 동작 중인 구성품들이 더 많으므로, 지난번에 데드락에 빠졌던 것과는 다른 **타이밍 프로필**에 실행될 가능성이 높다.== 그러므로 무조건 재부팅을 통해서만 시스템을 복구할 수 있는 데드락보다는 시간이 허락되면 복구가 가능한 라이브락이 낫다.

시스템을 올바르게 구축하기 위해 이 방법을 꼭 사용해야 하는 것은 아니다. 오히려 개발 및 테스트 중에 미처 점검하지 않은 타이밍 에러를 견딜 수 있는 시스템을 구축하기 위한 제안이다. 개인적으로는 시간 초과를 적절히 위치시켜 둘 것을 권하지만, 목표는 시간 초과가 발생하지 않도록 데드락이 없는 시스템에 두는 것이 좋다.

지금까지 언제 시간 초과를 활용할지를 조금이나마 이해했다. 이제 취소의 원인 및 취소를 정상적으로 처리할 수 있는 동시 프로세스를 구축하는 방법을 살펴보겠다. 동시 프로세스가 취소될 수 있는데는 여러 가지 이유가 있다.


1. 시간 초과

시간 초과는 암묵적인 취소다.

2. 사용자 개입

뛰어난 사용자 경험을 위해서는, ==일반적으로 장기간 실행되는 프로세스들을 동시에 시작한 다음 일정 시간 간격으로 사용자에게 상태를 보고하거나 사용자가 적합하다고 판단한 시점에 상태를 조회할 수 있도록 하는 것이 좋다.== 사용자와 대면하고 있는 동시 작업의 경우, 사용자가 시작한 작업을 취소할 수 있도록 허용해야 하는 상황도 있다.


3. 부모 프로세스의 취소

그와 관련하여 동시 작업의 부모(작업자 혹은 다른 무언가)가 멈추면 이 부모의 자식으로서, 자식은 취소될 것이다.


4. 복제된 요청

더 빠른 응답을 얻기 위해 하나의 프로세스에서 여러 동시 프로세스로 데이터를 보내려 할 수 있다. 최초의 응답이 돌아오면 나머지 프로세스를 취소하고자 할 것이다. 이에 대해서는 242페이지의 "복제된 요청"에서 자세히 설명한다.

다른 이유들도 가능할 수 있다. 그러나 "왜"라는 질문은 "어떻게"라는 질문만큼 어렵지 않고 흥미롭지도 않다. 4장에서는 동시 프로세스를 취소하는 두 가지 방법, 즉 done 채널과 Context 타입을 살펴봤고, 비교적 쉬운 내용이었다. 여기서는 보다 복잡한 질문을 알아보고자 한다. 하나의 동시 프로세스가 취소됐을 때 이것이 실행 중인 알고리즘 및 다음 단계의 소비자에게 어떤 의미가 있을까? 언제든지 종료될 수 있는 동시성 코드를 작성할 때 고려해야 할 사항은 무엇인가?

==이러한 질문에 답하기 위해 알아봐야 할 첫 번째는 동시 프로세스의 **선점 가능성**(preemptablillity)이다.== 다음과 같은 코드가 고루틴에서 실행된다고 가정해보자.


```go
var value any
select{
case <-done:
return
case value=<-valueStream:
}

result:=reallyLongCalculation(value)

select{
case <-done:
return
case resultStream<- result:
}
```

고루틴이 취소됐는지 확인하기 위해, valueStream의 읽기와 resultStream의 쓰기를 done 채널과  충실하게 연결했지만 여전히 문제가 있다. reallyLongCalculation은 선점 가능한 것으로 보이지 않으며, 그 이름을 보면 정말 오랜 시간이 걸릴 것 같다. 즉, reallyLongCalculation이 실행되는 동안 무언가가 이 고루틴을 취소하려고 하면 취소 및 중단을 알아채기까지 매우 오랜 시간이 걸릴 수 있다는 의미이다. reallyLongCalculation을 선점 가능하게 만들고 어떤 일이 일어나는지 살펴보자.


```go

    reallyLongCalculation:=func (
        done <-chan any,
        value any,
    ) any {
        intermediateResult:=logCalculation(value)

        select {
        case <-done:
            return nil
        default:            
        }

        return logCalculation(intermediateResult)
    }
```

약간의 진전을 이루었다. 이제 reallyLongCalCulation은 선점 가능하지만, 문제가 절반밖에 해결되지 않았다는 것을 알 수 있다. 겉보기에 오랫동안 실행될 것 같은 함수 호출들 사이에서만 reallyLongCalculation을 선점할 수 있다.


```go
reallyLongCalculation:=func(
done<-chan interface{},
value interface{},
){
intermediateResult:=longCalculation(done,value)
return longCalculation(done,intermediateResult)
}
```

이러한 논리적인 결론에 이르는 일련의 추론을 받아들인다면, 두 가지 작업을 해야 한다는 점을 알 수 있다. 동시 프로세스가 선점될 수 있는 기간을 정의하고, 이보다 많은 시간이 걸리는 기능은 자체적으로 선점 가능하다는 것을 보장해야 한다. 이를 위한 손쉬운 방법은 고루틴을 작은 조각으로 나누는 것이다. ==수용할 수 있다고 생각되는 기간내에 완료되지 않는, 모든 선점 불가능한 원자적 연산을 목표로 삼아야 한다.==

여기에는 또 다른 문제가 있다. 고루틴이 데이터베이스나 파일, 메모리 내의 데이터 구조체와 같은 공유 상태를 수정하는 경우 고루틴이 취소되면 어떻게 되는가? 해당 고루틴이 지금까지 수행된 중간 단계를 되돌리려고 시도하는가? 이 작업을 수행하는 데는 얼마나 걸리는가? 뭔가가 중단돼야 한다고 말했기 때문에 고루틴이 작업을 되돌리는 데 너무 오래 걸려서는 안 된다. 그렇지 않는가?

이 문제를 다루는 방법에 대해 일반적인 조언을 하는 것은 어렵다. 이 상황을 어떻게 다룰지에 대한 많은 부분이 알고리즘의 특성에 달려 있기 때문이다. 그러나 제한된 범위 내에서 공유 상태에 대한 수정 사항을 유지하거나 혹은 수정 사항이 쉽게 롤백되는 경우는 취소 처리할 수 있으며, 때에 따라 둘 모두를 수행할 수 있다. 가능하다면 중간 결과를 메모리에 구성한 다음 가능한 빠르게 상태를 수정하도록 하자. 다음은 이를 잘못 수행한 예제이다.

```go
result:=add(1,2,3)
writeTalllyToState(result)
result=add(reault,4,5,6)
writeTallyToState(result)
result=add(result,7,8,9)
writeTallyToState(result)
```

여기에서는 상태에 쓰는 코드가 세 번이나 나온다. 이 코드를 실행하는 고루틴이 마지막 쓰기 전에 취소된 경우 이전 두 번의 writeTallyToState 호출을 되돌리도록 작성해야 한다. 이 접근 방법을 다음과 비교해보자.

```go
result:=add(1,2,3,4,5,6,7,8,9)
writeTallyToState(result)
```

롤백과 관련해 신경 써야 하는 부분이 줄어들었다. 여전해 writeTallyToState를 호출한 후에 취소가 발생하면 변경 사항을 취소할 방법이 필요하지만, 상태를 한 번만 수정하기 때문에 이런 일이 발생할 가능성은 훨씬 적다.

신경 써야 할 또 다른 문제는 **중복 메시지**다. 생성기 단계와 A단계, B단계의 세 단계로 구성된 파이프라인이 있다고 가정해보자. 생성기 단계는 채널에서 마지막으로 읽은 이후 경과한 시간을 추적하는 방식으로 단계 A를 모니터링하고, 현재 인스턴스가 제대로 동작하지 않는다면 새로운 인스턴스인 A2를 생성한다. 그렇게 되면 B단계는 중복된 메시지를 받을 수 있다(그림 5-1).

[그림]

A단계가 이미 B단계에 결과를 보낸 후에, 취소 메시지가 나타나면 B단계가 중복 메시지를 받을 수 있음을 알 수 있다. 중복 메시지 전송을 막을 수 있는 방법은 몇 가지가 있다. 가장 쉬운(그리고 개인적으로 권장하는) 방법은 자식 고루틴이 이미 결과를 보고한 후 에 부모 고루틴이 취소 신호를 보낼 가능성이 거의 없도록 만드는 것이다. 이를 위해서는 각 단계 사이의 양방향 통신이 필요하다. 자세한 내용은 227 페이지의 "하트비트"를 참조한다. 다른 접근법은 다음과 같다.

###### 처음 또는 마지막으로 보고된 결과 수용

알고리즘의 반복 연산을 허용하는 경우나 또는 동시에 실행되는 프로세스들이 멱등(idempotent)인 경우에, 다음 단계의 하류 프로세스들에서는 중복 메시지가 올 가능성을 허용하고 첫 번째 또는 마지막 결과 중 하나를 수락할지 여부를 선택할 수 있다.


###### 부모 고루틴을 풀링(polling)해 권한 부여

명시적으로 메시지 전송 권한을 요청하기 위해 부모와의 양방향 통신을 사용할 수 있다. 뒤에서 살펴보겠지만 이 접근법은 하트비트와 유사하다. 이는 그림 [5-2] 처럼 보일 것이다.

[그림]


B의 채널에 쓰기를 수행할 수 있는 권한을 명시적으로 요청했기 때문에, 이는 하트비트보다 더 안전한 경로이다. 그러나 실제적으로 이 경로가 필요한 경우는 거의 없고, 하트비트보다 더 복잡하며, 하트비트가 더 일반적으로 유용하기 때문에 하트비트만 사용하는 것이 좋다.

==동시 프로세스를 설계할 때는 타임 아웃과 취소를 고려해야 한다.== 소프트웨어 엔지니어링의 다른 많은 주제와 마찬가지로, 시간 초과 및 취소를 처음부터 무시하다가 나중에서야 넣으려고 시도하는 것은 마치 케이크를 다 구운 다음 달걀을 추가하는 것과 같다.


### 하트비트

**하트비트**(heartbeat)는 동시 프로세스가 외부로 생존 신호를 보내는 방법이다. 하트비트라는 용어는 관찰자에게 살아 있음을 나타내는 인체 해부학의 심장 박동에서 그 이름을 따왔다. 하트비트는 Go 이전부터 있어왔고, 내부에 유용하게 남아 있다.

하트비트가 동시코드에서 흥미로운 몇 가지 이유가 있다. 하트비트는 시스템 내부를 보다 잘 이해할 수 있게 해 주며, ==하트비트가 없다면 결정론적으로 테스트할 수 없었을 시스템을 확률적인 변수 없이 결정론적으로 테스트할 수 있게 해 준다.==

이 절에서는 두 가지 유형의 하트비트를 다룬다.

- 일정 시간 간격으로 발생하는 하트비트
- 작업 단위의 시작 부분에서 발생하는 하트비트

일정 시간 간격으로 발생하는 하트비트는 , 하나의 작업 단위(unit of work)를 처리를 처리하기 위해 다른 일이 일어날 때까지 대기 중인 동시 코드에 유용하다. 고루틴은 이 일이 언제 시작될지 알지 못하기 때문에, 뭔가 일어나기를 기다리는 동안 잠시 빈둥거리고 있을 것이다. 하트비트는 모든 것이 잘되고 있으며, 그 침묵이 예상된 것이라는 사실을 리스너(listener)에게 알리는 방법이다.


다음 코드는 **하트비트를 노출하는 고루틴**을 보여준다.

```go
package main

import "time"

  

func main() {
    doWork := func(
        done <-chan any,
        pulseInterval time.Duration,
    )(<-chan interface{},<-chan time.Time) {

        heartbeat:=make(chan interface{}) // #1
        results:=make(chan time.Time)

        go func() {
            defer close(heartbeat)
            defer close(results)

            pulse:=time.Tick(pulseInterval) // #2
            workGen:=time.Tick(2*pulseInterval)// #3

  
            sendPulse:=func ()  {

                select {
                case heartbeat<-struct{}{}:
                default: // #4
                }
            }

  

            sendResult:=func (r time.Time)  {
                for {
                    select{
                    case <-done:
                        return
                    case <-pulse: // #5
                    sendPulse()
                    case results<-r:
                        return
                    }
                }
            }

            for {
                select {
                case <-done:
                    return
                case <-pulse:
                    sendPulse()
                case r:=<-workGen:
                    sendResult(r)
                }
            }
        }()
        return heartbeat,results
    }
}
```

1. 여기서는 하트비트를 보내기 위한 채널을 이용한다. doWork 외부로 이 채널을 리턴한다. 
2. 여기서 주어진 pulseInterval에서 하트비트가 뛰도록 설정한다. 모든 pulseInterval에는 이 채널에서 읽을 내용이 있다.
3. 이것은 들어오는 작업을 시뮬레이션하는 데 사용되는 또 다른 티커(ticker)이다. 고루틴에서 나오는 하트비트를 볼 수 있도록 pulseInterval보다 큰 지속 시간을 선택한다.
4. default 절이 포함돼 있다는 점에 유의한다. 언제나 아무도 하트비트를 듣지 않을 수 있다는 사실에 대비해야 한다. 고루틴에 나온 결과는 중요하지만, 규칙적인 펄스는 별로 중요하지 않다.
5. done 채널와 마찬가지로 송수신을 수행할 때마다 하트비트의 펄스에 대한 case 역시 포함시켜야 한다.

입력을 기다리는 동안이나 결과를 보내기 위해 기다리는 동안 여러 개의 펄스를 보낼 수 있기 때문에, 모든 select 문은 for 루프 내에 있어야 한다. 지금까지는 좋아 보인다. 이 함수를 어떻게 활용해야 하며, 이 함수가 내보내는 이벤트를 어떻게 소비해야 할까?
한번 살펴보자.

``` go
    done:=make(chan any)
    time.AfterFunc(10*time.Second,func() {close(done)}) // #1
    const timeout=2*time.Second // #2
    heartbeat,results:=doWork(done,timeout/2) // #3

  

    for {

        select {

        case _,ok:=<-heartbeat: // #4
            if ok==false {
                return
            }

            fmt.Printf("pulse")

        case r,ok:=<-results: // #5
            if ok==false {
                return
            }
            fmt.Printf("result %v\n",r.Second())

        case <-time.After(timeout): // #6
            return
        }
    }
```

1. 표준 done 채널을 설정하고 10초 후에 이를 닫는다. 이로써 고루틴이 일을 할 약간의 시간을 준다.
2. 여기서는 시간 초과 기간을 설정한다. 하트비트의 간격과 시간 초과를 연결하는 데 이 값을 사용한다.
3. 여기서는 timeout/2를 전달한다. 이것은 시간 초과가 너무 민감하게 이루어지지 않도록 하트비트가 응답할 수 있는 추가적인 틱(tick)을 제공한다.
4. 여기서는 heartbeat에 대한 select를 수행한다. 결과가 없으면 적어도 timeout/2마다 heartbeat 채널의 메시지를 확인한다. 메시지를 받지 못하면 고루틴 자체에 문제가 생겼음을 알 수 있다.
5. result 채널에서 select를 수행한다. 여기는 별 다를 것이 없다.
6. 하트비트나 새로운 결과 둘 모두를 받지 못한 경우 여기서 시간 초과된다.

이 코드의 실행 결과는 다음과 같다.

```shell
PS C:\Users\kgm09\goproject\src\other> go run main.go
pulse
pulse
result 58
pulse
pulse
result 0
pulse
pulse
result 2
pulse
pulse
result 4
pulse
pulse
```


###### 전체 코드
```go
package main

import (
    "fmt"
    "time"
)

  

func main() {

    doWork := func(
        done <-chan any,
        pulseInterval time.Duration,
   ) (<-chan interface{}, <-chan time.Time) {

        heartbeat := make(chan interface{}) // #1
        results := make(chan time.Time)

        go func() {
            defer close(heartbeat)
            defer close(results)
            pulse := time.Tick(pulseInterval)       // #2
            workGen := time.Tick(2 * pulseInterval) // #3

            sendPulse := func() {
                select {
                case heartbeat <- struct{}{}:
                default: // #4
                     }
            }

            sendResult := func(r time.Time) {
                for {
                   select {
                    case <-done:
                        return

                    case <-pulse: // #5
                        sendPulse()

                    case results <- r:
                        return
                    }
                }
            }
            for {

                select {
                case <-done:
                    return

                case <-pulse:
                    sendPulse()

                case r := <-workGen:
                    sendResult(r)
                }
            }
        }()
        return heartbeat, results
    }

    done := make(chan any)
    time.AfterFunc(10*time.Second, func() { close(done) }) // #1
    const timeout = 2 * time.Second                        // #2
    heartbeat, results := doWork(done, timeout/2)          // #3

    for {
        select {
        case _, ok := <-heartbeat: // #4
            if ok == false {
                return
            }
            fmt.Printf("pulse\n")
            
        case r, ok := <-results: // #5
            if ok == false {
                return
            }
            fmt.Printf("result %v\n", r.Second())

        case <-time.After(timeout): // #6
            return
        }
    }
}
```
의도한 대로 하나의 결과당 2개의 펄스 신호를 받았음을 알 수 있다.

제대로 작동하는 시스템에서는 하트비트가 그렇게 흥미롭지 않다. 유휴 시간과 관련된 통계를 수집하는 데 사용할 수는 있지만, 고정된 간격마다 발생하는(interval-based)하트비트의 활용도는 고루틴이 예상대로 작동되지 않을 때 실제로 빛이 난다.


다음 예제를 살펴보자. ==두 번의 반복 후에 패닉을 일으키며 멈추는 식으로 잘못 작성된 고루틴을 시뮬레이션하고, 두 채널 모두를 닫지 않은 상태로 둔다. 한번 살펴보자.==


```go
package main

  

import (
    "fmt"
    "time"
)

  

func main() {

doWork:=func (
    done<-chan interface{},
    pulseInterval time.Duration,
) (<-chan interface{},<-chan time.Time) {

    heartbeat:=make(chan interface{})
    results:=make(chan time.Time)

    go func() {
        pulse:=time.Tick(pulseInterval)
        workGen:=time.Tick(2*pulseInterval)

        sendPulse:=func ()  {
            select {
            case heartbeat<-struct{}{}:
            default:
            }
        }

        sendResult:=func (r time.Time)  {
            for  {
               select {
                case <-pulse:
                    sendPulse()
                case results<-r:
                    return
                }
            }
        }

  

        for i := 0; i < 2; i++ { // #1
            select {
            case <-done:

                return
            case <-pulse:
                sendPulse()

            case r:=<-workGen:
                sendResult(r)

            }
        }
  }()
    return heartbeat,results
}

done:=make(chan interface{})
time.AfterFunc(10*time.Second,func() {close(done)})

  

const timeout=2*time.Second
heartbeat,results:=doWork(done,timeout/2)

for {
    select {
    case _,ok:=<-heartbeat:
        if ok==false{
            return
        }
        fmt.Println("pulse")

    case r,ok:=<-results:
        if ok==false{
            return
        }
        fmt.Printf("results %v\n",r)

    case <-time.After(timeout):
        fmt.Println("worker goroutine is not healthy!")
        return
    }
}

  

}
```

1. 여기서는 패닉을 시뮬레이션한다. 앞의 예제처럼 멈추는 것을 요청하기 전까지 무한히 반복하는 대신, 2번만 루프를 반복한다. 실행 결과는 다음과 같다.
```shell

PS C:\Users\kgm09\goproject\src\other> go run main.go
pulse
pulse
worker goroutine is not healthy!
```

아름답다! 시스템은 2초 내에 고루틴이 뭔가 잘못되었다는 것을 깨닫고 for-select 루프를 벗어날 수 있다. 하트비트를 사용해 데드락을 성공적으로 회피했으며 상대적으로 더 긴 시간 초과에 의존할 필요가 없으므로 실행시간이 사전에 결정된다. 188 페이지의 "[[Go의 동시성 패턴#context 패키지|비정상 고루틴의 치료]]"이 개념을 더 자세히 받아들일 수 있는 방법에 대해 논의할 것이다.

또한 하트비트가 반대의 경우도 도움이 된다는 점에 주목하자. 값 채널에 보낼 값을 작업이 만들어내는데 오래 걸려서 오랫동안 실행되고 있는 고루틴이 있다면, 이 고루틴이 여전히 실행되고 있다는 사실을 우리에게 알려줄 것이다.


이제 작업 단위의 시작 부분에서 발생하는 하트비트를 살펴보겠다. 이들은 테스트에 매우 유용하다. ==다음은 매 작업 단위 앞에서 **펄스**를 보내는 예제이다.==


```go

package main

  

import (
    "fmt"
    "math/rand"
)

  

func main() {

  

    doWork := func(done <-chan interface{}) (<-chan interface{}, <-chan int) {
        heartbeatStream := make(chan interface{}, 1) // #1
        workStream := make(chan int)

        go func() {

            defer close(heartbeatStream)
            defer close(workStream)

            for i := 0; i < 10; i++ {

                select { // #2
                case heartbeatStream <- struct{}{}:
                default: // #3
                }

                select {
                case <-done:
                    return
                case workStream <- rand.Intn(10):
                }
            }
        }()
        return heartbeatStream, workStream
    }

    done := make(chan interface{})
    defer close(done)

    heartbeat, results := doWork(done)

    for {
        select {
        case _, ok := <-heartbeat:
            if ok {
                fmt.Println("pulse")
            } else {
                return
            }

        case r, ok := <-results:
            if ok {
                fmt.Printf("results %v\n", r)
            } else {
                return
            }
        }
    }
}
```

1. 여기서는 크기가 1인 버퍼로 heartbear 채널을 생성한다. 이렇게 히면 송신 대기 시간 내에 아무 채널을 듣고 있지 않아도 적오도 하나의 펄스가 송출된다.
2. 여기서 하트비트를 위한 별도의 select 블록을 설정한다. 수신자가 결과를 받을 준비가 되지 않았다면 결과 대신 펄스를 받게 되고, 현재의 결과 값을 잃어버리게 되므로, 이를 results 채널에 대한 전송과 동일한 select 블록에 포함시키지 않는다. 또한 default case가 있기 때문에 done 채널에 대한 case 구문을 넣지 않는다.
3. 다시 한번 아무도 하트비트를 듣지 않는다는 사실을 대비한다. heartbeat 채널은 크기가 1인 버퍼로 생성되었기 때문에, 누군가가 채널을 듣고 있지만 첫 번째 하트비트에 맞춰서 듣지 못하면 하트비트가 계속 통보된다.

코드의 실행 결과는 다음과 같다.

``` shell

PS C:\Users\kgm09\goproject\src\other> go run main.go
pulse
results 1
pulse
results 2
pulse
results 4
results 2
pulse
results 8
pulse
results 5
results 8
results 9
results 7
results 4
```

의도한 대로 결과를 하나 받을 때마다 펄스도 받았다는 것을 알 수 있다.

이 기법이 정말 빛을 발하는 시점은 **쓰기 테스트할 수행**할 때다. 고정 간격 기반의 하트비트는 동일한 방식으로 사용할 수 있으며, 고루틴이 자신의 작업을 시작했는지에만 관심이 있다면 이 스타일의 하트비트는 간단하다. 다음 코드 조각을 살펴보자.

```go

package main

import (
    "time"
)
func DoWork(
done <-chan interface{},
    nums ...int,
) (<-chan interface{}, <-chan int) {

    heartbeat := make(chan interface{}, 1)
    intStream := make(chan int)

    go func() {
        defer close(heartbeat)
        defer close(intStream)

        time.Sleep(2 * time.Second) // #1

  

        for _, n := range nums {
            select {
            case heartbeat <- struct{}{}:
            default:
            }

            select {
            case <-done:
                return
            case intStream <- n:
            }
        }
    }()
    return heartbeat, intStream
}
```

1. ==여기서는 고루틴이 작업을 시작할 수 있기 전에 약간의 자연을 시뮬레이션한다.== 실제 상황에서 무엇이든 이러한 지연을 일으킬 수 있으며 이는 비결정적이다. 개인적으로 CPU 부하, 디스크 경합, 네트워크 대기 시간, 그리고 도깨비로 인한 지연도 본적이 있다.


DoWork 함수는 넘겨준 숫자를 자신이 리턴하는 채널의 스트림으로 변환하는 매우 간단한 생성기다. 이 함수를 한번 테스트해보자. 다음은 나쁜 테스트의 예이다.

```go

  

func TestDoWork_GeneratesAllNumbers(t *testing.T) {
    done := make(chan interface{})
    defer close(done)


    intSlice := []int{0, 1, 2, 3, 5}
    _, results := DoWork(done, intSlice...)

  
    for i, expected := range intSlice {
    
        select {
        case r := <-results:
            if r != expected {
                t.Errorf(
                    "index %v: expected %v, but received %v,",
                    i,
                    expected,
                    r,
                )
            }

        case <-time.After(1 * time.Second): // #1
            t.Fatal("test timec out")
        }
    }
}
```

1. 여기서는 테스트가 데드락에 빠지는 것을 막기에 적당하다고 생각되는 기간에 시간 초과를 설정한다.


이 테스트의 실행 결과는 다음과 같다.

``` go
PS C:\Users\kgm09\goproject\src\other> go test pulse_test.go
--- FAIL: TestDoWork_GeneratesAllNumbers (1.01s)
    pulse_test.go:56: test timec out
FAIL
FAIL    command-line-arguments  2.205s
FAIL
```

이 테스트는 비결정적이기 때문에 좋지 않은 테스트이다. 예제 함수에서는 이 테스트가 항상 실패할 것이라 확신했지만, time.Sleep을 제거하려고 하면 상황이 악화된다. 이 테스트는 어떤 경우에는 통과하고 어떤 경우에는 실패한다.

앞에서 고루틴이 첫 번째 반복에 이르는 데 더 오래 걸리도록 하는 프로세스의 외부 요소를 설명했다. ==고루틴이 첫 번째로 스케줄돼 있는지 여부도 관심사다.== 요점은 시간 초과가 일어나기 전에 고루틴의 첫 번째 반복이 발생한다는 것을 확신할 수 없다는 것이므로, 확률에 대해 생각해보자. 이 시간 초과는 얼마나 중요한가? ==시간 초과를 늘릴 수는 있지만, 그렇게 하면 실패하는 데 더 많은 시간이 걸린다는 것을 의미한다.== ==따라서 전체적인 테스트 시간이 오래 걸릴 것이므로 테스트 모음의 속도가 느려진다.==

이는 끔찍한 상황이다. 팀은 테스트 실패를 신뢰해야 하는지 또는 무시해도 되는지를 더 이상 알지 못한다. 모든 노력이 흐트러지기 때문이다.

다행히도 하트비트로 이를 쉽게 해결할 수 있다. 다음은 사전에 실행 시간이 결정돼 있다는 예제이다.

```go

func TestDoWork_GeneratesAllNumbers(t *testing.T) {

    done := make(chan interface{})
    defer close(done)
    intSlice := []int{0, 1, 2, 3, 5} 

    heartbeat, results := DoWork(done, intSlice...)
    <-heartbeat // #1

    i := 0
    
    for r := range results {
        if expected := intSlice[i]; r != expected {
            t.Errorf("index %v:excepted %v,but received %v,", i, expected, r)
        }
        i++
    }
}
```

1. 여기서는 고루틴이 반복을 처리하기 시작했다는 신호를 보낼 때까지 대기한다.

이 테스트의 실행 결과는 다음과 같다.

```shell
PS C:\Users\kgm09\goproject\src\other> go test pulse_test.go
ok      command-line-arguments  3.392s
```

==하트비트 덕분에 시간 초과 없이 안전하게 테스트를 작성할 수 있다.== 유일한 운영상의 리스크는, 반복 작업 중 하나가 너무 많은 시간을 소비하는 것이다. 이 리스크가 중요하다면, 더 안전한 고정 간격 기반의 하트비트를 활용해 완벽한 안정성을 얻을 수 있다.

다음은 고정 간격 기반 하트비트를 사용한 예제이다.

```go

package other_test

import (
    "testing"
    "time"
)

  
func DoWork(
    done <-chan interface{},
    pulseInterval time.Duration,
    nums ...int,
) (<-chan interface{}, <-chan int) {

    heartbeat := make(chan interface{}, 1)
    intStream := make(chan int)

    go func() {
        defer close(heartbeat)
        defer close(intStream)

        time.Sleep(2 * time.Second)
        pulse := time.Tick(pulseInterval)

    numLoop: // #2
        for _, n := range nums {
            for { // #1
                select {
                case <-done:
                    return

                case <-pulse:
                    select {
                    case heartbeat <- struct{}{}:
                    default:
                    }

                case intStream <- n:
                    continue numLoop // #3
                }
            }
        }
    }()
    return heartbeat, intStream
}

  

func TestDoWork_GeneratesAllNumbers(t *testing.T) {
    done := make(chan interface{})
    defer close(done)

    intSlice := []int{0, 1, 2, 3, 5}
    const timeout = 2 * time.Second

    heartbeat, results := DoWork(done, timeout/2, intSlice...)

    <-heartbeat // #4

  

    i := 0
    for {
        select {
        case r, ok := <-results:
            if ok == false {
                return
            } else if expected := intSlice[i]; r != expected {
                t.Errorf("index %v: expected %v, but received %v,",
                    i,
                    expected,
                    r)
            }
            i++
  

        case <-heartbeat: // #5

        case <-time.After(timeout):
            t.Fatal("test timed out")
        }
    }
}
```

1. 두 개의 루프가 필요하다. 하나는 숫자 목록을 순화하는 것이며, 이 내부 루프는 숫자가 intStream에 성공적으로 전송될 때까지 실행된다.
2. ==여기에서 레이블을 사용해 내부 루프에서 continue하는 것을 조금 더 간단하게 만든다.==
3. 여기에서 외부 루프를 계속 실행(continue)한다.
4. 고루틴의 루프에 들어갔음을 나타내는 첫 번째 하트비트가 발생할 때까지 계속 기다린다.
5. 또한 시간 초과가 발생하지 않도록 여기에서 하트비트를 select한다.

이 테스트를 실행한 결과는 다음과 같다.

```shell
PS C:\Users\kgm09\goproject\src\other> go test pulse_test.go
ok      command-line-arguments  4.760s
```

이 버전의 테스트가 훨씬 더 불명확하다는 것을 눈치챘을지도 모르겠다.== 예제에서 테스트하고 있는 논리가 약간 혼란스럽다.== 이런 이유로 (고루틴의 루프가 시작되면 실행을 멈추지 않는다고 논리적으로 확신한다면) ==첫 번째 하트비트를 받았을 때 중단한 다음, 간단한 range 구문으로 들어갈 것을 추천한다.== ==채널을 닫지 못하거나 루프 반복이 너무 오래 걸리거나 다른 타이밍 관련 문제가 있는지 테스트하는 별도의 테스트를 작성할 수도 있다.==


하트비트가 동시 코드 작성 시 엄격하게 필요한 것은 아니지만, 이 절에서는 그 용도를 설명했다. 오랫동안 실행되는 고루틴 또는 테스트가 필요한 고루틴의 경우는 이 패턴을 사용하는 것이 좋다.


### 복제된 요청

일부 애플리케이션의 경우, 가능한 빨리 응답을 받는 것이 최우선 과제다. 예를 들어, 애플리케이션이 사용자의 HTTP 요청을 처리중이거나 복제된 데이터를 가져오는 중일 수 있다. 이러한 경우에는 일종의 교환을 할 수 있다. 여러 핸들러(고루틴, 프로세스 또는 서버)에 요청을 복제할 수 있으며, 그 중 하나는 다른 핸들러보다 빠르게 리턴될 것이고 그러면 즉시 결과를 리턴할 수 있을 것이다. 이 방법의 단점은 리소스를 활용해 핸들러의 여러 복사본을 계속 실행해야 한다는 것이다.

이 복제가 메모리 내에서 수행되면 비용이 많이 들지는 않지만, 핸들러를 복제할 때 프로세스, 서버 또는 데이터 센터를 복제해야 하는 경우 비용이 많이 든다. 이러한 비용이 실제로 이익이 되는지 결정해야 한다.

단일 프로세스 내에서 요청을 복제하는 방법을 살펴보자. 요청의 핸들러 역할을 위해 여러 고루틴들을 사용할 것이고, ==각 고루틴은 작업의 부하를 시뮬레이션하기 위해 1-6초 사이의 임의의 시란 동안 잠들 것이다.== 이렇게 하면 다양한 시간에 결과를 리턴하는 핸들러를 제공할 수 있으며, 이것이 결과를 더 빠르게 얻을 수 있는 방법이라는 사실을 입증할 수 있다.

다음은 10개의 핸들러로 시뮬레이션된 요청을 복제하는 예제이다.


``` go

package main

  

import (
    "fmt"
    "math/rand"
    "sync"
    "time"
)

func main() {

    doWork := func(
        done <-chan interface{},
        id int,
        wg *sync.WaitGroup,
        result chan<- int,
    ) {

        started := time.Now()
        defer wg.Done()

        // 랜덤한 작업 부하 시뮬레이션
        simulatedLoadTime := time.Duration(1+rand.Intn(5)) * time.Second
        select {
        case <-done:
        case <-time.After(simulatedLoadTime):
        }

        select {
        case <-done:
        case result <- id:
        }

        took := time.Since(started)
        // 핸들러가 얼마나 오래 걸리는지 표시
        if took < simulatedLoadTime {
            took = simulatedLoadTime
        }

        fmt.Printf("%v took %v\n", id, took)
    }

    done := make(chan interface{})
    result := make(chan int)

  
    var wg sync.WaitGroup
    wg.Add(10)

    for i := 0; i < 10; i++ {
        go doWork(done, i, &wg, result) // #1
    }

    firstReturned := <-result // #2
    close(done)               // #3
    wg.Wait()

  
    fmt.Println("Received an answer from #%v\n", firstReturned)

}
```


1. 요청을 처리할 핸들러 10개를 시작한다.
2. 여러 개의 핸들러 중에서 처음으로 리턴된 값을 받는다.
3. 나머지 핸들러들을 모두 취소시킨다. 이렇게 함으로써 그들이 더 이상 필용하지 않은 작업을 하지 않게 한다.

이 코드의 실행 결과는 다음과 같다.


``` shell
PS C:\Users\kgm09\goproject\src> go run main.go
5 took 1.0141434s
3 took 4s
9 took 1.0141434s
4 took 1.0141434s
0 took 2s
2 took 4s
8 took 4s
6 took 4s
7 took 2s
1 took 2s
Received an answer from #5 // 5번으로부터 응답을 받는다.
```


이 실행에서 5번째 핸들러가 가장 빠르게 리턴한 것처럼 보인다. 이 출력은 얼마나 많은 시간을 절약할 수 있는지 알 수 있도록 각 핸들러가 얼마나 오랫동안 기다렸는지를 보여준다. 핸들러 하나만 뽑았는데 그것이 2번 핸들러라고 하자. 요청을 처리하기 위해 잠깐 기다리는 것이 아니라 4초동안 기다려야 한다.

이 접근법에서 유일한 주의 사항은 모든 핸들러에게 요청을 처리할 동등한 기회가 필요하다는 것이다. 다시 말해서, 요청을 처리할 수 없듯이 그 핸들러가 아무리 빠르게 요청을 응답한다 해도 받을 수 없다는 것이다. 앞서 말했듯이, 핸들러가 자신의 작업을 수행하는데 사용하는 자원이 무엇이건 간에 그 자원을 복제해야 한다.

이 동일한 문제의 또 다른 증상은 획일성이다. 핸들러가 서로 너무 비슷하면 어떤 핸들러가 처리하든 이상치가 나올 가능성이 더 적다. ==그러므로 다른 프로세스, 기기, 데이터 저장소에 대한 경로 또는 서로 다른 데이터 저장소에 대한 접근 등 런타임 조건이 다른 핸들러로만 이러한 요청을 복제해야 한다.==

이 기법은 설치 및 유지보수 비용이 비싸지만 속도가 목적일 때 유용하다. 또한 자연스럽게 내결함성과 확장성을 제공한다.


### 속도 제한

서비스용 API로 작업해본 적이 있다면 속도 제한과 씨름했을 가능성이 높다. 속도 제한이란, ==리소스에 대한 접근을 단위 시간당 특정 횟수로 제한하는 것을 의미한다.== 이때 리소스는 API 연결, 디스크 읽기/쓰기, 네트워크 패킷, 에러 등 어떤 것도 될 수 있다.

서비스에 속도 제한을 적용하는 이유가 궁금했던 적 있는가? 시스템에 자유로운 접근을 허용하지 않는 이유가 뭘까? 가장 확실한 대답은 시스템에 속도 제한을 도입함으로써 시스템에 대한 전체적인 공격 벡터 클래스를 차단한다는 것이다. 리소스가 허용하는 한 빠르게 시스템에 접근할 수 있으면 악의적인 사용자는 모든 종류의 작업을 수행할 수 있다.

예를 들어, 공격자는 로그 메시지나 유효한 요청으로 서비스의 디스크를 채울 수 있다. 로그 순환(rotation)을 잘못 구성했다면, 공격자는 악의적인 작업을 수행할 수 있으며, 활동에 대한 기록이 로그에서 `/dev/null`로 순환될 만큼 충분한 요청을 할 수 있다. 공격자는 리소스에 대한 무차별적 접근을 시도할 수도 있고, 분산 서비스 거부 공격을 수행할 수도 있다. 요점은, 시스템에 대한 요청의 속도를 제한하지 않으면 쉽게 보안을 설정할 수 없다는 것이다.

악의적 사용이 속도 제한의 유일한 이유는 아니다. 분산 시스템에서는 합법적인 사용자가 너무 많은 양의 작업을 수행하거나 버그가 있는 코드를 연습하는 경우에도 다른 사용자의 시스템 성능을 저하시킬 수 있다. 또한 이 때문에 이전에 논의했던 **죽음의 나선**이 발생할 수도 있다. 제품의 관점에서 보자면 이는 끔찍한 문제다. 일반적으로 사용자가 어떤 정도의 성능을 기대할 수 있는지 일관된 기준을 보장하고자 할 것이다. 한 사용자가 이러한 합의에 영향을 미칠 수 있다면 좋지 않을 것이다. 사용자는 일반적으로 시스템에 대한 접근이 샌드박스로 돼 있어 다른 사용자의 활동에 영향을 미치지도 영향을 받지도 않는다고 생각한다. 그 생각의 틀을 깨면 사용자는 시스템이 잘 설계되지 않는 것처럼 느껴질 수 있으며, 사용자가 화를 내거나 떠날 수도 있다.


사용자가 단 한명이라도 속도를 제한하는 것이 나을 수 있다. 많은 경우 시스템은 일반적인 유즈 케이스에서 잘 작동하도록 개발되었지만, 상황에 따라 다르게 작동하기 시작할 수 있다. 분산된 시스템과 같은 복잡한 시스템에서 이 영향이 시스템 전체로 파급할 수 있으며, 극단적이고 의도하지 않았던 결과로 초래할 수 있다. 어쩌면 부하가 상태에서 패킷을 삭제하기 시작할지도 모른다. 이로 인해 분산 데이터베이스가 쿼럼(quorum, 완료됐다고 여겨지는 작업에 반드시 응답해야 하는 서버의 수)을 잃어버리고, 쓰기를 허용하지 않게 돼 기존 요청이 실패하게 되며, 결국 뭔가 잘못된 일이 일어날 수 있음을 알 수 있다. 시스템이 이런 식으로 스스로에게 DDos 공격을 수행하는 것이 전례가 없는 일도 아니다!

>[!note]-  실제사례
>새로운 프로세스로 시작해 작업을 병렬로 확장하는, 여러 개의 기기로 **수평 확장 가능한 분산 시스템**에서 작업한 적이 있다. 각 프로세스는 데이터베이스 연결을 열고, 일부는 데이터를 읽고 일부는 연산을 수행한다. 한동안은 고객의 요구를 충족시키기 위해 이러한 방식으로 시스템을 확장하는데 큰 성공을 거두었다. 그러나 얼마 후 시스템 사용률은 데이터베이스에서 읽는 시간이 초과될 정도까지 증가했다.
>당시 데이터베이스 관리자는 로그를 조사해 무엇이 잘못됐는지 파악했다. 그리고 마침내 시스템이 어떤 것에 대해서도 속도 제한이 설정되지 않았기 때문에 프로세스가 서로 충돌하고 있음을 발견했다. 디스크 경합은 100%로 급상승하고, 다른 프로세스가 디스크의 다른 부분에서 데이터를 읽으로겨 시도할 때까지 그 상태로 유지된다. 이로 인해 일종의 자학적인 라운드 로빈 시간 초과-재시도 루프가 발생한다. 작업은 결코 완료되지 않을 것이다.
>우리는 가능한 데이터베이스 연결 수를 제한하기 위한 시스템을 고안했다. 그리고 속도 제한도 도입해 클라우드 읽기 연결에서 초당 비트 수를 제한에 문제를 해결했다. 고객들은 작업이 완료됄 때까지 더 오래 기다려야 했지만 작업은 어쨌든 완료됐으며, 우리는 시스템의 용량을 구조화된 방법으로 확장하기 위해 적절한 용량을 계획할 수 있게 된다.


속도 제한은 시스템이 사전에 조사한 경계를 벗어나는 것을 막음으로써, 시스템의 성능과 안정성을 판단할 수 있도록 해 준다. 이러한 경계를 확장해야 하는 경우, 많은 테스트를 마친 후에 통제된 방식으로 경계를 확장할 수 있다.

시스템에 대한 접근에 비용을 청구하는 시나리오에서는 속도 제한을 통해 고객과 건전한 관계를 유지할 수 있다. 속도가 심하게 제한된 상황에서 시스템을 사용해볼 수도 있다. 구글은 자사의 클라우드를 이런 식으로 제공해 큰 성공을 거두었다.

구글이 고객에게 요금 지불을 요청하면, 속도 제한을 통해 사용자를 보호할 수 있다. 시스템에 대한 대부분의 접근은 프로그래밍 방식으로 이루어지므로, 유로 시스템에 제어할 수 없는 형태로 버그가 발생할 수 있다. 이 버그 때문에 비용이 많이 들 수 있으며, 여러 방안 중 어떤 방안을 채택해야 할지 결정해야 하는 어색한 상황에 처하게 된다.
예를 들어, 서비스의 주인이 비용을 부담하고 의도하지 않은 접근을 용납할지, 아니면 사용자가 청구서를 지불하도록 할지 결정해야 한다. 사용자에게 비용을 청구하는 경우 영구적으로 관계가 악화될 수 있다.

속도 제한은 종종 제한될 수 있는 자원을 만드는 사람들의 관점에서 생각할 수 있지만, 사용자가 속도 제한을 활용할 수도 있다. 서비스 API를 활용하는 방법을 이해하고 있다면, 속도 제한을 조정할 수 있어 매우 기분이 좋을 것이고 내 발등을 찍는 일을 피할 수 있을 것이다.

다행히도 절대 도달하지 못할 것으로 생각되는 한계를 설정하더라도, 속도를 제한하는 것이 좋다는 것이 좋다는 것을 확신시키기에 충분한 정당성을 부여했다. 속도 제한은 매우 간단하게 만들 수 있으며, 많은 문제를 해결하므로 사용하지 않을 이유가 없다.


Go에서도 속도 제한을 구현하려면 어떻게 해야 하는가?

대부분의 속도 제한은 **토큰 버킷**(token bucket)이라는 알고리즘을 통해서 이루어진다. 이는 이해하기 쉽고 구현하기도 비교적 쉽다. 배경이 되는 이론을 먼저 살펴보겠다.

리소스를 활용하려면 리소스에 대한 접근 토큰이 있어야 한다고 가정해보자. 토큰이 없으면 요청은 거부된다. 이제 이 토큰이 사용을 위해 검색 대기 중인 버킷에 저장되어 있다고 가정하자. 버킷을 깊이는 d이며, 이는 버킷이 한 번에 d개의 접근 토큰을 보유하고 있음을 나타낸다. 예를 들어 버킷의 길깊이가 5인 경우 다섯 개의 토큰을 보유할 수 있다.

이제 리소스에 접근해야 할 때마다 버킷으로 가서 토큰 하나 제거한다. 버킷에 토큰이 다섯 개 있는데 리소스에 다섯 번 접근하려고 하면, 접근할 수 있다. 하지만 여섯 번 째 시도에서는 사용할 수 있는 접근 토큰이 없을 것이다. 토큰이 사용 가능해질 때까지 요청을 대기열에 넣어 두거나 요청을 거부해야 한다.

이 개념을 시각화하는데 도움이 되는 시간표가 있다. 시간은 초 단위의 시간 차이(time-delta)를 나타내며, ==버킷은 버킷 내의 요청 토큰 수를 나타내고, 요청 열의 tok은 요청의 성공을 나타낸다.==(이 시간표 및 이후의 시간표에서는 시각화를 단순화하기 위해 요청은 즉각적이라고 가정한다.)


| 시간  | 버킷  | 요청  |
| --- | --- | --- |
| 0   | 5   | tok |
| 0   | 4   | tok |
| 0   | 3   | tok |
| 0   | 2   | tok |
| 0   | 1   | tok |
| 0   | 0   |     |
| 1   | 0   |     |
|     |     |     |

==첫 1초가 지나가기도 전에 다섯 번의 요청을 모두 할 수 있다는 것을 알 수 있으며==, 더 이상 사용 가능한 토큰이 없으므로 대기한다.

지금까지는 꽤 간단하다.== 토큰을 보충하는 것은 어떨까?== 항상 새 토큰을 얻을까? 토큰 버킷 알고리즘에서는 토큰이 버킷에 다시 추가하는 속도를 **r**로 정의한다. 이 값은 1나노초일 수도 있고 1분일 수도 있다. 이것이 일반적으로 생각하는 속도 제한이 된다. 새 토큰을 사용할 수 있을 때까지 기다려야하기 때문에 작업은 이 재충전 속도에 의해 제한된다.

다음은 깊이가 1이고 초당 토큰 1개가 회전되는 토큰 버킷의 예제이다.


| 시간  | 버킷  | 요청  |
| --- | --- | --- |
| 0   | 1   |     |
| 0   | 0   | tok |
| 1   | 0   |     |
| 2   | 1   |     |
| 2   | 0   | tok |
| 3   | 0   |     |
| 4   | 1   |     |
| 4   | 0   | tok |


즉시 요청을 할 수 있지만, 곧 2초에 한 번으로 요청이 제한되는 것을 볼 수 있다. 속도 제한이 제대로 동작한다.

이제는 얼마나 많은 토큰을 바로 사용할 수 있는지를 나타내는 버킷의 깊이 d와, 토큰이 보충되는 속도인 r이라는 두 가지 설정을 할 수 있다. 이 둘을 통해 간헐적 대량 요청의 최대치(burstiness)와 전반적인 속도 제한을 모두 제어할 수 있다. 간헐적 대량의 요청의 최대치란, 단순히 버킷을 가득 채웠을 때 얼마나 많은 요청을 할 수 있는지를 의미한다.

다음은 깊이가 5이고, 초당 0.5개의 토큰이 회전되는 토큰 버킷의 예이다.

| 시간  | 버킷     | 요청  |
| --- | ------ | --- |
| 0   | 5      |     |
| 0   | 4      | tok |
| 0   | 3      | tok |
| 0   | 2      | tok |
| 0   | 1      | tok |
| 0   | 0      | tok |
| 1   | 0(0.5) |     |
| 2   | 1      |     |
| 2   | 0      | tok |
| 3   | 0(0.5) |     |
| 4   | 1      |     |
| 4   | 0      | tok |
|     |        |     |


이번에는 즉시 5개의 요청을 할 수 있으며, 이 이후에 2초에 하나의 요청으로 제한된다. 간헐적 대량 요청(burst)은 시작 부분에서 발생했다.

사용자가 하나의 긴 스트림에서 토큰 버킷 전체를 사용하지 못할 수 있다는 점에 유의하자. 버킷의 깊이는 버킷의 용량만 제어한다. 다음은 처음에 2개의 대량 요청, 그리고 4초 후에 5개의 대량 요청을 가진 사용자의 예이다.


| 시간  | 버킷  | 요청  |
| --- | --- | --- |
| 0   | 5   |     |
| 0   | 4   | tok |
| 0   | 3   | tok |
| 1   | 3   |     |
| 2   | 4   |     |
| 3   | 5   |     |
| 4   | 5   |     |
| 5   | 4   | tok |
| 5   | 3   | tok |
| 5   | 2   | tok |
| 5   | 1   | tok |
| 5   | 0   | tok |
|     |     |     |


사용자가 쓸 수 있는 토큰을 가지고 있기만 하다면, 순간 최대치는 시스템에 대한 접근을 허용하면 호출자의 용량에 의해서만 제한한다. ==간헐적으로만 시스템에 접근하고, 작업을 끝내자마자 가능한 빠르게 돌아가고자 하는 사용자는 간헐적 대량 요청을 활용하는 것이 좋다.== 동시에 집중되는 모든 사용자의 요청 시스템에서 처리할 수 있는지 또는 충분한 사용자가 시스템에 영향을 미치기 위해 동시에 요청을 집중시킬 가능성이 통계적으로 거의 없는지만 확인하면 된다. 어느 쪽이든, 속도 제한은 사전에 계산된 리스크만 감당할 수 있게 해 준다.

이 알고리즘을 사용해 토큰 버킷 알고리즘의 구현에 대해 작성된 Go 프로그램이 어떻게 작동하는지 살펴보자.

API에 접근하는 척 하면서, Go 클라이언트가 API를 활용하도록 제공한다. 이 API에는 두 개의 엔드 포인트가 있다. 하나는 파일 읽기 용이고 다른 하나는 도메인 이름을 IP 주소로 해석하는 용도다. 단순화를 위해 실제로 서비스에 접근하는 데 필요한 인수와 리턴 값은 제거할 것이다. 그러므로 우리 클라이언트는 다음과 같다.

```go

package main

  

import "context"

  

type APIConnection struct{}
  

func Open() *APIConnection {
    return &APIConnection{}
}

func (a *APIConnection) ReadFile(ctx context.Context) error {
    // 여기서 작업하는 척 한다.
    return nil
}

  

func (a *APIConnection) ResolveAddress(ctx context.Context)error  {
    // 여기서 작업하는 척 한다.
    return nil
}
```


이론적으로 이 요청은 네트워크를 통해 이루어지므로, 요청을 취소하거나 서버에 값을 전달해야 할 경우를 대비해 context.Context를 첫 번째 인수로 사용한다. 굉장히 표준적인 내용이다.

이제 이 API에 접근할 수 있는 간단한 드라이버를 만들 것이다. 이 드라이버는 10개의 파일을 읽고 10개의 주소를 확인해야 하는데, 파일과 주소는 서로 연관이 없으므로 드라이버는 이러한 API 호출을 통해 동시에 수행할 수 있다. 이러한 사항은 나중에 APIClient에 부하를 가하고 속도 제한을 사용하는데 도움이 된다.

``` go
func main() {

    defer log.Printf("Done.")
    log.SetOutput(os.Stdout)
    log.SetFlags(log.Ltime | log.LUTC)

  

    apiConnection := Open()
    var wg sync.WaitGroup

    wg.Add(20)

  

    for i := 0; i < 10; i++ {

        go func() {
            defer wg.Done()
            err := apiConnection.ReadFile(context.Background())
            if err != nil {
                log.Printf("cannot ReadFile: %v", err)
            }

            log.Printf("ReadFile")
        }()

    }

  

    for i := 0; i < 10; i++ {

        go func() {
            defer wg.Done()
            err := apiConnection.ResolveAddress(context.Background())
            if err != nil {
                log.Printf("cannot ResolveAddress: %v", err)
            }
            log.Printf("ResolveAddress")
        }()
    }
    wg.Wait()

}
```

이 코드의 실행 결과는 다음과 같다.

``` shell

PS C:\Users\kgm09\goproject\src> go run main.go
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 ResolveAddress
19:35:05 ReadFile
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ResolveAddress
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 ReadFile
19:35:05 Done.
```

모든 API 요청이 거의 동시에 처리되는 것을 볼 수 있다. 속도 제한 설정이 없으므로 고객이 원하는 만큼 자수 시스템에 접근할 수 있다. 이제는 드라이버에 무한 루프를 발생시킬 수 있는 버그가 있음을 알려줄 시점이 됐다. 만약 속도 제한이 없다면, 기본 나쁜 청구서를 받았을지도 모를 일이다.

그러면 이제 속도 제한을 도입해보자! 여기서는 APIConnection 내에 속도 제한을 도입할 것이지만,일반적으로 속도 제한은 서버에서 실행돼 사용자가 쉽게 우회할 수 없다. 운영 시스템에서는 클라이언트가 거부당할 것이 불필요한 호출을 하지 않도록 클라이언트 측 속도 제한을 포함시킬 수도 있지만 이는최적화에 해당한다. 예제의 목정을 위해 클라이언트 측 속도 제한은 단순하게 유지한다.

`golang.org/x/time/rate` ==패키지의 토큰 버킷 속도 제한기 구현을 사용하는 예제를 살펴보자.== 이 패키지는 표준 라이브러리에 가깝기 때문에 이 패키지를 선택했다. 더 많은 신호를 주고받으면서 동일한 일을 하는 다른 패키지가 분명 존재하며, 실제 운영 시스템에서 사용하기에는 더 좋은 서비스일 수도 있다. 그러나 `golang.org/x/time/rate` 패키지는 매우 간단하므로 우리의 목적에는 더 잘 부합한다.

이 패키지와 상호작용할 수 있는 두 가지 방법은 Limit 타입과 NewLimiter 함수다. 다음을 살펴보자.

``` go

package main

  
// Limit는 어떤 이벤트의 최대 빈도를 정의한다.
// Limit는 초당 이벤트의 수를 나타낸다. 0의 Limit는 아무런 이벤트도 허용하지 않는다.

  
type Limit float64

// NewLimiter은 새로운 r의 속도를 가지며 최대 b개의 토큰을 가지는
// 새로운 Limiter를 리턴한다.

func NewLimiter(r Limit,b int) *Limit
```


NewLimiter에서는 익숙한 매개 변수인 r과 b를 볼 수 있다. r은 앞에서 논의한 속도이며, b은 앞에서 논의한 버킷 깊이다.

rate 패키지는 time.Duration을 Limit로 변환하는 것을 돕기 위해 도우미 메서드인 Every를 정의한다.

```go

// Every는 Limit에 대한 이벤트 사이의 최소 시간 간격을 반환한다.
func Every(interval time.Duration) Limit
```

Every 함수는 의미가 있지만, 여기서는 속도 제한을 요청 간의 간격이 아니라 **측정 시간당 작업 수**라는 면에서 논의하고자 한다. 이를 다음과 같이 나타낼 수 있다.


``` go
rate.Limit(events/timePeriod.Seconds())
```

하지만 이를 매번 입력하고 싶지는 않다. 또한 Every 함수는 주어진 interval이 0이면 rate.Inf를 리턴하는 특수한 논리를 가지고 있다. 따라서 자체 도우미 함수를 Every함수로 표현할 것이다.

``` go

func Per(eventCount int, duration time.Duration) rate.Limit {

    return rate.Every(duration / time.Duration(eventCount))
}
```


rate.Limiter를 생성하고 난 후, 접근 토큰이 주어질 때까지 요청을 대기하도록 하는 데 이를 사용할 것이다. 이를 위해 Wait 함수를 사용하면 된다. 이 함수는 단순히 1을 매개변수로 WaitN를 호출해준다.

```go
// Wait는 WaitN(ctx,1)의 축약형이다.

func (lim *rate.Limiter) Wait(ctx context.Context)  


// WaitN 함수는 lim가 n개의 이벤트 발생을 허용할 때까지 대기한다.
// n이 Limiter의 버퍼 사이즈를 초과하면 error를 리턴하며, Context는 취소된다.
// 그렇지 않은 경우에는 Context의 Deadline이 지날 때까지 대기한다.

func (lim *rate.Limiter) WaitN(ctx context.Context,n int) (err error)
```

 이제 우리의 API 요청에 속도 제한을 시작하는데 필요한 모든 재료를 가지고 있다. APIConnection 타입을 수정하고 테스트해보자!


``` go

  

type APIConnection struct {
    rateLimiter *rate.Limiter
}

  

func Open() *APIConnection {
    return &APIConnection{
        rateLimiter: rate.NewLimiter(rate.Limit(1), 1),
    }
}

  

func (a* APIConnection) ReadFile(ctx context.Context)error  {
    if err :=a.rateLimiter.Wait(ctx);err!=nil {
        return err
    }

    // 여기서 작업하는 척한다.
    return nil
}

  

func (a *APIConnection) ResolveAddress(ctx context.Context) error  {

    if err := a.rateLimiter.Wait(ctx);err!=nil {
        return err
    }

    // 여기서 작업하는 척한다.
    return nil
}
```

1. 여기서 모든 API 연결에 대해 1초당 1개의 이벤트라는 속도 제한을 설정한다.
2. 여기서는 속도 제한에 의해 요청을 완료하기에 충분한 접근 토큰을 가질 때까지 대기한다.

코드의 실행 결과는 다음과 같다.

```shell

PS C:\Users\kgm09\goproject\src> go run main.go
22:59:16 ReadFile
22:59:17 ReadFile
22:59:18 ReadFile
22:59:19 ResolveAddress
22:59:20 ReadFile
22:59:21 ReadFile
22:59:22 ReadFile
22:59:23 ReadFile
22:59:24 ReadFile
22:59:25 ReadFile
22:59:26 ReadFile
22:59:27 ResolveAddress
22:59:28 ResolveAddress
22:59:29 ResolveAddress
22:59:30 ResolveAddress
22:59:31 ResolveAddress
22:59:32 ResolveAddress
22:59:33 ResolveAddress
22:59:34 ResolveAddress
22:59:35 ResolveAddress
22:59:35 Done.
```

이전에는 동시에 모든 API 요청을 처리하는 반면, 이번에는 한 번에 한 개의 요청씩 완료한다. 우리의 속도 제한이 제대로 작동하는 것처럼 보인다.

이것은 매우 기본적인 속도 제한을 제공하지만, 운영 시에는 좀 더 복잡한 것을 원할 수 있다. 아마도 초당 요청의 수를 제한하는 세분화된 컨트롤과 분당, 시간당 또는 일별 요청을 제한하는 큰 규모의 컨트롤을 여러 계층 설정해야 한다.

어떤 경우에는 하나의 속도 제한기를 사용해 이를 수행할 수도 있다. 그러나 모든 경우에 이것이 가능하지는 않으며, 시간 단위별 제한의 의미를 하나의 계층으로 말아버리면 속도 제한의 의도와 관련된 많은 정보를 잃게 된다. 이런 이유로 속도 제한기를 분리된 상태로 유지하면서, 여러 개의 속도 제한기를 하나의 속도 제한기로 결합해 상호 작용을 관리하는 것이 더 간단하다는 사실을 발견했다. 이런 관점에서 multiLimiter라는 간단한 통합된 속도 제한기를 만들었다. 그 정의는 다음과 같다.

``` go

package main

  

import (

    "context"
    "sort"
    "golang.org/x/time/rate"

)

  

type RateLimiter interface { // #1
    Wait(context.Context) error
    Limit() rate.Limit
}

type multiLimiter struct{
    limiters []RateLimiter
}

func MultiLimiter(limiters ...RateLimiter)*multiLimiter  {

    byLimiter:=func (i,j int) bool  {
        return limiters[i].Limit()< limiters[j].Limit()
    }

    sort.Slice(limiters,byLimiter) // #2

    return &multiLimiter{limiters: limiters}

}

  

func (l *multiLimiter) Wait(ctx context.Context) error {

    for _, l := range l.limiters {

        if err :=l.Wait(ctx);err!=nil {

            return  err

        }

    }

    return nil

}

  

func (l *multiLimiter) Limit() rate.Limit {

    return l.limiters[0].Limit()// #3

}
```

1. 여기서 RateLimiter interface를 정의했으며, 이로 인해 MultiLimiter는 다른 MultiLimiter 인스턴스를 재귀적으로 정의할 수 있다.
2. 여기서는 최적화를 구현하고 각 RateLimiter의 Limit() 정렬한다.
3. multiLimiter가 인스턴스화될 대 자식 RateLimiter 인스턴스들을 정렬하기 때문에, 간단하게 가장 한정적인 제한을 리턴할 수 있다. 슬라이스의 첫 번째 요소가 이에 해당한다.

==Wait 메서드는 루프를 통해 모든 자식 속도 제한기를 순회하면서 각 자식 속도 제한기를 호출한다.== 이러한 호출은 대기할 수도 있고 아닐 수도 있다. 그러나 요청의 각 속도 제한기에 통지해야만 토큰 버킷을 줄일 수 있다. 각각의 속도 제한에 따라 대기하게 되면 결과적으로 가장 긴 대기 시간 동안 기다리게 된다. 그 이유는 상대적으로 짧은 대기 시간들은 가장 긴 대기 시간에 포함되고, 짧은 대기들을 마치고 남은 시간만큼만 대기하도록 가장 긴 대기 시간을 다시 계산할 것이기 때문이다. 버킷 앞쪽의 짧은 대기로 인해 기다리는 동안 긴 대기 시간은 버킷을 다시 채우기 있기 때문에 대기 시간이 조정되는 것이며, 그 후에 대기들은 즉시 리턴된다.


이제 속도 제한들로부터 속도 제한을 표시할 수 있는 방법이 생겼으므로 이를 수행해보자. APIConnection를 초당 제한과 분당 제한을 가지도록 제정의한다.

``` go

package main

  

import (

    "context"
    "log"
    "os"
    "sync"
    "time"
    "golang.org/x/time/rate"

)

// Limit는 어떤 이벤트의 최대 빈도를 정의한다.
// Limit는 초당 이벤트의 수를 나타낸다. 0의 Limit는 아무런 이벤트도 허용하지 않는다.

type Limit float64

  
func Per(eventCount int, duration time.Duration) rate.Limit {
    return rate.Every(duration / time.Duration(eventCount))
}

type APIConnection struct {
    rateLimiter RateLimiter
}

  

func Open() *APIConnection {
    SencondLimit := rate.NewLimiter(Per(2, time.Second), 1)  // #1
    minuteLimit := rate.NewLimiter(Per(10, time.Minute), 10) // #2
    return &APIConnection{
        rateLimiter: MultiLimiter(SencondLimit, minuteLimit), // #3
    }
}

  

func (a *APIConnection) ReadFile(ctx context.Context) error {
    
    if err := a.rateLimiter.Wait(ctx); err != nil {
        return err
    }
    // 여기서 작업하는 척한다.
    return nil
}

  

func (a *APIConnection) ResolveAddress(ctx context.Context) error {

    if err := a.rateLimiter.Wait(ctx); err != nil {
        return err
    }
    // 여기서 작업하는 척한다.
    return nil
}

  

func main() {

    defer log.Printf("Done.")
    log.SetOutput(os.Stdout)
    log.SetFlags(log.Ltime | log.LUTC)

    apiConnection := Open()
    var wg sync.WaitGroup

    wg.Add(20)

    for i := 0; i < 10; i++ {

        go func() {
            defer wg.Done()
            err := apiConnection.ReadFile(context.Background())

            if err != nil {
                log.Printf("cannot ReadFile: %v", err)
            }
            log.Printf("ReadFile")
        }()
    }

  

    for i := 0; i < 10; i++ {

        go func() {
            defer wg.Done()
            err := apiConnection.ResolveAddress(context.Background())
            if err != nil {
                log.Printf("cannot ResolveAddress: %v", err)
            }
            log.Printf("ResolveAddress")
        }()
    }
    wg.Wait()
}
```

1. 여기서는 대량 요청을 처리하지 않는 초당 속도 제한을 정의한다.
2. 여기서는 사용자에게 초기 풀(pool)을 제공하기 위해 10개의 대량 요청을 처리할 수 있는 분당 속도 제한을 정의한다. 초당 제한을 통해 시스템에 요청 과부하가 이루어지지 않게 할 수 있다.
3. 그런 다음 두 제한을 결합하여 APIConnection의 주 속도 제한기로 설정한다.

이 코드의 실행 결과는 다음과 같다.

``` shell
PS C:\Users\kgm09\goproject\src> go run main.go united_speed_limit.go
01:54:02 ReadFile
01:54:03 ReadFile
01:54:03 ReadFile
01:54:04 ReadFile
01:54:04 ReadFile
01:54:05 ReadFile
01:54:05 ReadFile
01:54:06 ReadFile
01:54:06 ResolveAddress
01:54:07 ReadFile
01:54:08 ResolveAddress
01:54:14 ResolveAddress
01:54:20 ResolveAddress
01:54:26 ResolveAddress
01:54:32 ResolveAddress
01:54:38 ResolveAddress
01:54:44 ResolveAddress
01:54:50 ResolveAddress
01:54:56 ResolveAddress
01:55:02 ReadFile
01:55:02 Done.
```

여기서 볼 수 있듯이 앞 부분에는 초당 2건을 요청한다. 이 시점부터 6초마다 요청하기 시작한다. 이것은 부당 요청 토큰의 사용 가능한 풀을 모두 소모해 초당 속도 제한에 의해 제한을 받기 때문이다.

11번째 요청이 이후의 나머지 요청들처러 6초 후에 이루어지는 것이 아니라, 2초 후에 발생하는 이유가 다소 직관적이지 않을 수 있다. API 요청은 1분당 10번으로 제한되지만, 그 시간의 범위는 **시간이 지남에 따라 이동**(sliding window)한다. 11번째 요청에 도달했을때 1분이 지났기 때문에 분당 속도 제한기는 새로운 토큰을 하나 얻게 된다.

이와 같은 제한을 정의하면 미세한 수준으로 요청을 제한하면서도 대단위의 한계를 명확하게 표현할 수 있다.

이 기법을 사용하면 시간 이외의 다른 차원에서 생각할 수 있다. 시스템의 속도를 제한하면 아마 두 가지 이상을 제한하게 될 것이다. ==API 요청의 수에 제한이 있을 수 있지만 디스크 접근, 네트워크 접근 등과 같은 다른 리소스에 대한 제한도 있을 것이다.== 예제에 조금 더 살을 붙이고 디스크 및 네트워크의 속도 제한을 설정해보자.

``` go
type Limit float64

  

func Per(eventCount int, duration time.Duration) rate.Limit {
    return rate.Every(duration / time.Duration(eventCount))
}

  

type APIConnection struct {
    networkLimit,
    diskLimit,
    apiLimit RateLimiter
}

  

func Open() *APIConnection {

    return &APIConnection{
        apiLimit: MultiLimiter( // #2
            rate.NewLimiter(Per(2, time.Second), 2),
            rate.NewLimiter(Per(10, time.Minute), 10),
        ),

        diskLimit: MultiLimiter( // #3
            rate.NewLimiter(rate.Limit(1), 1),
        ),

        networkLimit: MultiLimiter(// #4
            rate.NewLimiter(Per(3, time.Second), 3),
        ),
    }
}

  

func (a *APIConnection) ReadFile(ctx context.Context) error {

    err:=MultiLimiter(a.apiLimit,a.diskLimit).Wait(ctx) // #4

    if err != nil {
        return err
    }
    // 여기서 작업하는 척 한다.
    return nil
}

  

func (a *APIConnection) ResolveAddress(ctx context.Context) error {

    err:=MultiLimiter(a.apiLimit,a.networkLimit).Wait(ctx) // #5

    if err != nil {
        return err
    }
    return nil
}
```

1. 여기서는 API 호출에 대한 속도 제한을 설정한다. 초당 요청 수 및 분당 요청 수에 대한 제한이 있다.
2. 여기서는 디스크 읽기에 대한 속도 제한을 설정한다. 초당 1번의 읽기로 제한한다.
3. 네트워크에 대해서는 초당 3번의 읽기로 제한한다.
4. 파일을 읽으려고 하면 API 호출에 대한 제한과 디스크 읽기에 대한 제한이 조합될 것이다.
5. 네트워크에 접근하려고 하면 API제한기와 네트워크 제한기가 조합될 것이다.


이 코드의 실행 결과는 다음과 같다.
```shell
PS C:\Users\kgm09\goproject\src> go run main.go u_speedlimit.go
11:40:10 ReadFile
11:40:10 ResolveAddress
11:40:11 ReadFile
11:40:11 ResolveAddress
11:40:11 ResolveAddress
11:40:12 ResolveAddress
11:40:12 ReadFile
11:40:13 ResolveAddress
11:40:13 ReadFile
11:40:14 ResolveAddress
11:40:16 ReadFile
11:40:22 ResolveAddress
11:40:28 ResolveAddress
11:40:34 ResolveAddress
11:40:40 ReadFile
11:40:46 ReadFile
11:40:52 ReadFile
11:40:58 ReadFile
11:41:04 ResolveAddress
11:41:10 ReadFile
11:41:10 Done.
```

여기에서 또 다른 시간표를 만들어 각 호출이 왜 일어나는지 분석할 수는 있지만, 그러면 핵심을 놓치게 될 것이다. 그 대신 논리적 속도 제한기들을 각 호출에 맞게 그룹으로 구성할 수 있으며, APIConnection이 제대로 작업을 수행한다는 사실에 초점을 맞추도록 하겠다. APIConnection의 동작에 대해 간략하게 설명하자면, 네트워크 접근과 관련된 API 호출이 좀 더 규칙적인 것처럼 보이며, 처음 2/3 부분에서 끝나는 것을 볼 수 있다. 이는 고루틴들이 스케줄링돼 있는 것과 관련이 있을 수도 있지만, 속도 제한기가 제대로 동작하는 쪽이 훨씬 더 관련이 있다.

또한 rate.Limiter 타입이 최적화 및 다양한 사용 사례를 위해 몇 가지 특별한 트릭을 가지고 있다는 점도 언급해야 겠다. 토큰 버킷이 또 다른 토큰을 받을 때까지 기다릴 수 있는 기능에 대해서만 설명했지만, 이를 사용하는 데 관심이 있다면 몇 가지 다른 기능이 있다는 점도 알아두자.

이 절에서는 속도 제한을 활용해야 하는 이유 및 그 구축 알고리즘, Go의 토큰 버킷 알고리즘 구현, 여러 개의 토큰 버킷 제한기를 더 크고 복잡한 속도 제한기로 구성하는 방법에 대해 살펴봤다. 이는 속도 제한에 대한 개요를 알려주며, 현장에서 사용하기 시작하는 데 도움이 된다.


---
# 비정상 고루틴의 치료

데몬과 같이 수명이 긴 프로세스에서는 수명이 긴 고루틴의 집합을 사용하는 것이 일반적이다. 이러한 고루틴들은 보통 어떤 식으로든 데이터가 들어오기를 기다리면서 멈춰 있다가, 데이터가 오면 깨어나서 작업을 수행한 다음 데이터를 전달한다. ==때때로 고루틴은 제대로 제어하지 못하는 리소스에 의존한다.== 어쩌면 고루틴이 웹 서비스에서 데이터를 가져오라는 요청을 받거나, 임시 파일을 모니터링하는 중일 수도 있다. ==요점은 고루틴이 외부의 도움없이 복구할 수 없는 나쁜 상태에 빠지기 쉽다는 것이다.== 만약 관심사를 분리한다고 하면, 나쁜 상태에서 스스로 회복하는 방법을 알아내기 위한 작업을 수행하는 것이 고루틴 관심사가 되어서는 안 된다고까지 말할 수 있다. ==장기간 실행되는 프로세스에서는, 고루틴이 건강한 상태인지 확인하고 건강한 상태가 아니라면 재시작하는 메커니즘을 만드는 것이 도움이 될 수 있다.== 고루틴을 다시 시작하는 과정을 "치료"라고 한다.[^2]


고루틴을 치료하기 위해, 모니터링하는 고루틴의 생존 여부를 확인하려는 목적으로 **하트비트 패턴**을 사용한다. ==하트비트 유형은 모니터링하려는 내용에 따라 결정되지만, 고루틴이 라이브락에 빠질 수 있는 경우==, ==하트비트에 고루틴이 살아 있을 뿐만 아니라 유용한 작업도 수행하고 있음을 나타내는 정보가 포함되도록 하라.== 이 절에서는 단순화를 위해 고루틴의 작동 여부만을 고려한다.

고루틴의 건강 상태를 모니터하는 논리를 스튜어드(steward, 관리인)라고 부를 것이며 모니터링 되는 고루틴을 와드(ward, 피후견인)라고 부를 것이다. ==스튜어드는 와드의 고루틴이 비정상 상태가 되면 해당 고루틴이 재시작되도록 해야 할 책임이 있다.== 이를 위해 해당 고루틴을 시작시킬 수 있는 함수에 대한 참조가 필요하다. 스튜어드가 어떤 형태를 가지는지 살펴보자.


``` go

package main

  

import "time"

  

type StartGoroutinFn func(done <-chan interface{}, pulseInterval time.Duration)(heartbeat <-chan interface{}) // #1

newSteward:=func (
        timeout time.Duration,
        startGorutine StartGoroutinFn,
    ) StartGoroutinFn { // #2

        return func(done <-chan interface{}, pulseInterval time.Duration) ( <-chan interface{}) {

            heartbeat:=make(chan interface{})
            go func() {
                defer close(heartbeat)

                var wardDone chan interface{}
                var wardHeartbeat <-chan interface{}

                startWard:=func ()  { // #3
                    wardDone=make(chan interface{}) // #4
                    wardHeartbeat=startGorutine(or(wardDone,done),timeout/2) // #5
                }

                startWard()
                pulse:=time.Tick(pulseInterval)

                monitorLoop:

                for{
                    timeoutSignal:=time.After(timeout)

                    for{
                        select {
                        case <-pulse:
                            select {
                            case heartbeat <-struct{}{}:
                            default:
                            }
                        case <-wardHeartbeat: // #7
                        continue monitorLoop
                        case <-timeoutSignal: // #8
                        log.Println("steward: ward unhealthy; restarting")
                        close(wardDone)
                        startWard()
                        continue monitorLoop
                        case <-done:
                            return
                        }
                    }
                }
            }()

            return heartbeat
        }
    }

```

1. 여기서는 모니터링되고 재시작될 고루틴의 시그니처를 정의한다. 하트비트 패턴에서 친숙한 done 채널 및 pulseInterval과 heartbeat를 볼 수 있다.
2. 이 행에서 스튜어드가 모니터링 대상이 되는 고루틴에 대한 timeout과, 모니터링하는 고루틴을 실행하기 위한 startGoroutine 함수를 인자로 받는 것을 볼 수 있다. ==흥미롭게도 스튜어드는 startGoroutine을 리턴하는데, 이는 스튜어드 자체도 역시 모니터링 가능하다는 것을 나타낸다.==
3. 여기서는 정형화된 방식으로 우리가 모니터링할 고루틴을 시작시킬 클로저(closure)를 작성한다.
4. 여기서는 중단 신호를 보내야 할 경우를 대비해, 피후견인 고루틴으로 전달할 새로운 채널을 만든다.
5. 여기서 모니터링할 고루틴을 시작한다. 스튜어드가 멈추거나 혹은 스튜어드가 와드 고루틴을 멈추게 하고자 하는 경우에 와두 고루틴이 멈추기를 원하기 때문에 논리적인 or로 두 가지 done 채널을 감싸준다. 전달한 pulseInterval은 시간 초과 기간의 절반이지만, 227 페이지의 [[확장에서의 동시성#하트비트|하트비트]]에서 이야기했듯이 이 값은 조정 가능하다.
6. 이것은 스튜어드가 자체적으로 펄스를 보낼 수 있도록 하는 내부 루프이다.
7. 여기서 와드의 하트비트를 받으면 계속 모니터링 루프를 진행한다.
8. 이 행은 시간 초과 기간 내에 와드로부터 펄스를 받지 못하면 와드를 중단시키고 새로운 와드 고루틴이 시작되도록 요청한다는 것을 나타낸다. 모니터링을 계속한다.

for 루프는 다소 바쁘다. 하지만 관련된 패턴을 잘 알고 있으면 비교적 쉽게 읽을 수 있다. 스튜어드를 테스트 삼아 실행해보자. 오작동하는 고루틴을 모니터링하면 어떻게 될까?

한번 살펴보자.


``` go


    log.SetOutput(os.Stdout)
    log.SetFlags(log.Ltime|log.LUTC)


    doWork:=func (done <-chan interface{}, _ time.Duration) <-chan interface{} {
        log.Println("ward: Hello, I'm irresponsible!")
        go func() {
            <-done // #1
            log.Println("ward: I am halting.")
        }()
        return nil
    }

    doWorkWithSteward:=newSteward(4*time.Second,doWork) // #2

    done:=make(chan interface{})
    time.AfterFunc(9*time.Second,func() { // #3
        log.Println("main: halting steward and ward.")
        close(done)
    })


    for range doWorkWithSteward(done,4*time.Second) {} // #4
        log.Println("Done")

}
```


1. 여기서는 이 고루틴이 아무것도 하지 않고 취소되기만을 기다린다는 것을 알 수 있다. 이 고루틴은 아무 펄스도 보내지 않는다.
2. 이 행은 doWork 고루틴의 시작시키는 스튜어드를 생성하는 함수를 만든다. doWork에 대한 시간 초과는 4초로 설정했다.
3. 9초 후에 스튜어드와 와드 모두를 중단시켜 예제를 끝낼 것이다.
4. 마지막으로 예제가 멈추는 것을 막기 위해 스튜어드를 시작시키고 그 펄스들을 순회하기 시작한다.

예제 실행결과는 다음과 같다.

``` shell
PS C:\Users\kgm09\goproject\src> go run main.go startGoroutinFn.go
11:36:50 ward: Hello, I'm irresponsible!
11:36:54 steward: ward unhealthy; restarting
11:36:54 ward: Hello, I'm irresponsible!
11:36:54 ward: I am halting.
11:36:58 steward: ward unhealthy; restarting
11:36:58 ward: Hello, I'm irresponsible!
11:36:58 ward: I am halting.
11:36:59 main: halting steward and ward.
11:36:59 ward: I am halting.
11:36:59 Done
```

###### 전체코드

``` go
package main

  

import (
    "log"
    "os"
    "time"
)

  

func main() {

    var or func(channels ...<-chan interface{}) <-chan interface{}

    or = func(channels ...<-chan interface{}) <-chan interface{} { // <1>
        switch len(channels) {
        case 0: // <2>
            return nil
        case 1: // <3>
            return channels[0]
        }

  

        orDone := make(chan interface{})

        go func() { // <4>
            defer close(orDone)

            switch len(channels) {
            case 2: // <5>
                select {
                case <-channels[0]:
                case <-channels[1]:
                }
            default: // <6>
                select {
                case <-channels[0]:
                case <-channels[1]:
                case <-channels[2]:
                case <-or(append(channels[3:], orDone)...): // <6>
                }
            }
        }()
        return orDone
    }

    type startGoroutineFn func(
        done <-chan interface{},
        pulseInterval time.Duration,
    ) (heartbeat <-chan interface{}) // <1>


    newSteward := func(
        timeout time.Duration,
        startGorutine StartGoroutinFn,
    ) StartGoroutinFn { // #2

        return func(done <-chan interface{}, pulseInterval time.Duration) <-chan interface{} {
            heartbeat := make(chan interface{})

            go func() {
                defer close(heartbeat)

                var wardDone chan interface{}
                var wardHeartbeat <-chan interface{}

                startWard := func() { // #3
                    wardDone = make(chan interface{})                            // #4
                    wardHeartbeat = startGorutine(or(wardDone, done), timeout/2) // #5
                }

                startWard()
                pulse := time.Tick(pulseInterval)

            monitorLoop:
                for {
                    timeoutSignal := time.After(timeout)

                    for {
                        select {
                        case <-pulse:
                            select {
                            case heartbeat <- struct{}{}:
                            default:
                            }

                        case <-wardHeartbeat: // #7
                            continue monitorLoop
                        case <-timeoutSignal: // #8
                            log.Println("steward: ward unhealthy; restarting")

                            close(wardDone)
                            startWard()
                            continue monitorLoop
                        case <-done:
                            return
                        }
                    }
                }
            }()
            return heartbeat
        }
    }

    log.SetOutput(os.Stdout)
    log.SetFlags(log.Ltime | log.LUTC)

  

    doWork := func(done <-chan interface{}, _ time.Duration) <-chan interface{} {
        log.Println("ward: Hello, I'm irresponsible!")
        go func() {
            <-done // #1
            log.Println("ward: I am halting.")
        }()
        return nil
    }

    doWorkWithSteward := newSteward(4*time.Second, doWork) // #2
  

    done := make(chan interface{})
    time.AfterFunc(9*time.Second, func() { // #3
        log.Println("main: halting steward and ward.")
        close(done)
    })

    for range doWorkWithSteward(done, 4*time.Second) {
    } // #4
    log.Println("Done")
}
```

제대로 동작하는 것처럼 보인다. 와드는 조금 단순하긴 하다. 취소와 하트비트에 필요한 것 외에는 매개 변수를 필요로 하지도 않고 아무런 인자도 리턴하지 않는다. ==스튜어드와 함께 사용할 수 있는 형태를 가진 와드를 어떻게 만들 수 있을까?== 매번 와드에 맞게 스튜어드를 재작성하거나 생성할 수 있지만, 이 일은 번거롭고 불필요하다. 대신 클로저를 사용할 것이다. 불연속적인 값의 목록을 기반으로 정수 스트림을 생성할 와드를 살펴보자.


``` go

  

    doWorkFn:=func (

        done <-chan interface{},
        intList ... int,

    ) (startGoroutineFn,<-chan interface{}) { // #1

        intChanStream:=make(chan (<-chan interface{})) // #2
        intStream:=bridge(done,intChanStream)

        doWork:=func (
            done <-chan  interface{},
            pulseInterval time.Duration,

        ) <-chan interface{} { // #3

            intStream:=make(chan interface{}) // #4
            heartbeat:=make(chan interface{})

            go func() {
                defer close(intStream)
                select{
                case intChanStream<-intStream: // #5
                case <-done:
                    return
                }

                pulse:=time.Tick(pulseInterval)


                for{
                    valueLoop:
                    for _, intVal := range intList {
                        if intVal<0{
                            log.Println("negative value: %v\n",intVal) // #6
                            return
                        }

                        for{
                            select {
                            case <-pulse:
                                select {
                                case heartbeat <-struct{}{} :
                                default:
                                }

                            case intStream <-intVal:
                                continue valueLoop
                            case <-done:
                                return
                            }
                        }
                    }
                }
            }()
            return heartbeat
        }
        return doWork,intStream
    }
```

1. 여기서는 와드가 감싸기를 원하는 값을 받아서, 와드가 통신하는 데 사용하게 될 모든 채널을 리턴할 것이다.
2. 이 행은 브리지 패턴의 일부로 채널들의 채널을 생성한다.
3. 여기서 스튜어드가 시작시키고 감시할 클로저를 만든다.
4. 여기에서 와드의 고루틴 인스턴스 내에서 통신할 채널을 인스턴스화한다.
5. 여기서 브리지에게 통신할 새로운 채널에 대해 알려준다.
6. 이 행은 음수를 만났을 때 에러를 로깅하고 고루틴으로부터 리턴함으로써 비정상 상태의 와드를 시뮬레이션한다.


와드의 사본을 여러 개 실행할 것이기 때문에, doWork의 소비자에게 하나의 중단되지 않는 채널을 제공할 수 있도록 도와주는 브리지 체널(176 페이지의 "[[Go의 동시성 패턴#bridge 채널|bridge 채널]]" 참조)을 사용한다. 이러한 기법을 사용하면, 패턴들을 구성을 통해 간단하게 와드를 복합적으로 만들 수 있다. 이를 어떻게 활용하는지 살펴보자.


``` go
    log.SetFlags(log.Ltime | log.LUTC)

    log.SetOutput(os.Stdout)

  

    done := make(chan interface{})

    defer close(done)

  

    doWork, intStream := doWorkFn(done, 1, 2, -1, 3, 4, 5)      // #1

    doWorkWithSteward := newSteward(1*time.Millisecond, doWork) // #2

    doWorkWithSteward(done, 1*time.Hour)                        // #3

  

    for intVal := range take(done, intStream, 6) { // #4

        fmt.Printf("Received: %v\n", intVal)

    }
```

1. 이 행은 와드 함수를 생성하는데, 이 함수는 가변 정수 슬라이스를 감싸고 다시 통신할 스트림을 리턴할 수 있게 한다.
2. 여기서는 doWork 클로저를 모니터링할 스튜어드를 만든다. 상당히 빠르게 실패할 것이라 예상하기 때문에 모니터링 기간을 1밀리초로 설정한다.
3. 여기서는 스튜어드에게 와드를 시작하고 모니터링을 시작하려고 한다.
4. 마지막으로, 개발한 파이프라인 단계 중 하나를 사용하고 intStream으로부터 첫 6개의 값을 가져온다. 이 코드의 실행 결과는 다음과 같다

``` shell

PS C:\Users\kgm09\goproject\src> go run main.go
Received: 1
Received: 2
00:55:13 negative value: -1
00:55:13 steward: ward unhealthy; restarting
Received: 1
Received: 2
00:55:13 negative value: -1
00:55:13 steward: ward unhealthy; restarting
Received: 1
Received: 2
```

- 실행 결과

``` shell

Received: 1
23:25:33 negative value: -1
Received: 2
23:25:33 steward: ward unhealthy; restarting
Received: 1
23:25:33 negative value: -1
Received: 2
23:25:33 steward: ward unhealthy; restarting
Received: 1
23:25:33 negative value: -1
Received: 2
```


수신한 값들 속에 흩어져 있는, 와드로부터의 에러를 볼 수 있으며, 스튜어드는 에러를 탐지하고 와드를 재시작시킨다. 또한 1과 2만 수신된다는 것을 알 수 있다. 이는 처음부터 매번 나타나는 와드 증상이다. 와드를 개발할 때, 시스템이 중복된 값에 민감하다면 이를 고려해야 한다. 또한 특정 횟수만큼 실패한 이후에는 종료되도록 스튜어드를 작성하는 것도 생각해볼 수 있다. 이 경우에 반복 시마다 클로저가 감싸고 있는 intList를 업데이트함으로써 간단하게 생성기가 상태를 유지하게(stateful) 만들 수 있다. 이전과 비교해 보자.


``` go
valueloop:

for _, intVal:=range intList{
// ...
}
```

이 대신 다음과 같이 작성할 수 있다.

``` go
for{
intVal:=intList[0]
intList=intList[1:]
// ...
}
```

이렇게 하면 잘못된 음수에 계속 머물러 있게 되고 와드가 계속 실패할 수는 있지만, 와드가 다시 시작되는 동안에도 상태가 저장될 것이다.

이 패턴을 사용하면 장기간 실행되는 고루틴의 상태를 정상적으로 유지하면서 지속적으로 실행되도록 할 수 있다.



[^2]: 얼랭(Erlang)에 익숙한 사람이라면 이 개념을 알아볼 것이다. 얼랭의 감독관(supervisor)역시 동일한 역할을 한다.